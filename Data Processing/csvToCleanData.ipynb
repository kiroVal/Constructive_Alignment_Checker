{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb40c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install required packages and imports\n",
    "# Run this cell first\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(\"All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c94e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configuration and Settings\n",
    "# Define your file paths and header mappings\n",
    "\n",
    "# CHANGE THESE PATHS TO MATCH YOUR SETUP\n",
    "INPUT_CSV_DIRECTORY = \"/content/csv_outputs\"          # Where your CSV files from DOCX extraction are\n",
    "OUTPUT_DIRECTORY = \"/content/cleaned_csv\"             # Where you want cleaned files to go\n",
    "COMBINED_OUTPUT_DIR = \"/content/combined_csv\"         # Where you want the final combined file\n",
    "\n",
    "# Define canonical headers and their possible variations (based on your DOCX extraction)\n",
    "HEADER_MAPPING = {\n",
    "    \"Learning Outcomes\": [\n",
    "        \"Learning Outcomes\", \"Learning Outcome\", \"Learning\\\\nOutcomes\",\n",
    "        \"Learning Outcomes (LO) / Learner and Learning Outcomes (LLO)\",\n",
    "        \"Learning Outcomes\\\\n(At the end of the session, students are expected to:)\",\n",
    "        \"Learning Outcomes\\\\n(At the end of the session, students are expected to :)\",\n",
    "        \"Learning\\\\nOutcomes\", \"Learning \\\\nOutcomes\"\n",
    "    ],\n",
    "    \"Deliverables\": [\n",
    "        \"Deliverables Outcomes\", \"Deliverables/Outcomes\", \"Deliverables\", \n",
    "        \"Deliverables/\\\\nOutcomes\", \"Deliverables\\\\n/ Outcomes\", \"Deliverables/ Outcomes\",\n",
    "        \"Deliverables/\\\\xa0 Outcomes\", \"Deliverables/\\\\xa0\\\\nOutcomes\", \n",
    "        \"Deliverable / Outcomes\", \"Deliverables\\\\n/ Outcomes/Rubrics\"\n",
    "    ],\n",
    "    \"Assessments\": [\n",
    "        \"Assessments\", \"Assessment\", \"Assessment Task\", \"Assessment\\\\n/ Output\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(COMBINED_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {INPUT_CSV_DIRECTORY}\")\n",
    "print(f\"Output directory: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"Combined output directory: {COMBINED_OUTPUT_DIR}\")\n",
    "print(\"Configuration completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400707fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define helper functions\n",
    "# These functions will be used in the main processing steps\n",
    "\n",
    "def normalize_header(text):\n",
    "    \"\"\"\n",
    "    Normalize header text: lowercase, remove spaces, slashes, punctuation.\n",
    "    Same logic as your DOCX extraction script.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
    "\n",
    "def find_csv_files(directory):\n",
    "    \"\"\"Find all CSV files in the specified directory.\"\"\"\n",
    "    csv_pattern = os.path.join(directory, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    return csv_files\n",
    "\n",
    "def match_headers_to_canonical(df_headers, header_mapping):\n",
    "    \"\"\"\n",
    "    Match DataFrame headers to canonical headers using substring matching.\n",
    "    Returns matched canonical headers, column indices, and final header names.\n",
    "    \"\"\"\n",
    "    matched_canonical_headers = []\n",
    "    col_indices = []\n",
    "    final_headers = []\n",
    "    \n",
    "    # Normalize input headers\n",
    "    normalized_df_headers = [normalize_header(h) for h in df_headers]\n",
    "    \n",
    "    for canonical_header, possible_variations in header_mapping.items():\n",
    "        found_variation_index = -1\n",
    "        \n",
    "        # Check if any variation matches as substring\n",
    "        for variation in possible_variations:\n",
    "            normalized_variation = normalize_header(variation)\n",
    "            \n",
    "            for j, normalized_table_header in enumerate(normalized_df_headers):\n",
    "                if normalized_variation in normalized_table_header or normalized_table_header in normalized_variation:\n",
    "                    found_variation_index = j\n",
    "                    break\n",
    "                    \n",
    "            if found_variation_index != -1:\n",
    "                break\n",
    "        \n",
    "        if found_variation_index != -1:\n",
    "            col_indices.append(found_variation_index)\n",
    "            final_headers.append(canonical_header)\n",
    "            matched_canonical_headers.append(canonical_header)\n",
    "            \n",
    "    return matched_canonical_headers, col_indices, final_headers\n",
    "\n",
    "def clean_text_data(df):\n",
    "    \"\"\"Clean text data removing common DOCX extraction artifacts.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            # Strip whitespace\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            \n",
    "            # Replace 'nan' string with empty string\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN'], '')\n",
    "            \n",
    "            # Clean up common formatting issues from DOCX extraction\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\n', ' ', regex=False)\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\xa0', ' ', regex=False)\n",
    "            df_clean[col] = df_clean[col].str.replace('  +', ' ', regex=True)  # Multiple spaces to single\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "print(\"Helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze your CSV files (Optional but recommended)\n",
    "# This helps you understand what headers are in your files\n",
    "\n",
    "def analyze_csv_headers(input_directory):\n",
    "    \"\"\"Analyze headers across all CSV files.\"\"\"\n",
    "    csv_files = find_csv_files(input_directory)\n",
    "    header_analysis = {}\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to analyze\")\n",
    "    print(\"Analyzing headers...\")\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, nrows=0)  # Read only headers\n",
    "            filename = os.path.basename(file_path)\n",
    "            header_analysis[filename] = list(df.columns)\n",
    "        except Exception as e:\n",
    "            filename = os.path.basename(file_path)\n",
    "            header_analysis[filename] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return header_analysis\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Starting header analysis...\")\n",
    "headers = analyze_csv_headers(INPUT_CSV_DIRECTORY)\n",
    "\n",
    "# Show sample of headers from first 5 files\n",
    "print(f\"\\nTotal files found: {len(headers)}\")\n",
    "print(\"\\nSample headers from first 5 files:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "count = 0\n",
    "for filename, file_headers in headers.items():\n",
    "    if count < 5:\n",
    "        if isinstance(file_headers, list):\n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"  Headers: {file_headers}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"{filename}: {file_headers}\")\n",
    "            print()\n",
    "        count += 1\n",
    "\n",
    "# Check which files have the target headers\n",
    "print(\"Checking for target headers in all files...\")\n",
    "target_header_count = {\"Learning Outcomes\": 0, \"Deliverables\": 0, \"Assessments\": 0}\n",
    "\n",
    "for filename, file_headers in headers.items():\n",
    "    if isinstance(file_headers, list):\n",
    "        normalized_headers = [normalize_header(h) for h in file_headers]\n",
    "        \n",
    "        # Check each canonical header\n",
    "        for canonical_header, variations in HEADER_MAPPING.items():\n",
    "            found = False\n",
    "            for variation in variations:\n",
    "                normalized_variation = normalize_header(variation)\n",
    "                for norm_header in normalized_headers:\n",
    "                    if normalized_variation in norm_header or norm_header in normalized_variation:\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "            if found:\n",
    "                target_header_count[canonical_header] += 1\n",
    "\n",
    "print(\"\\nTarget header distribution:\")\n",
    "for header, count in target_header_count.items():\n",
    "    print(f\"  {header}: found in {count}/{len(headers)} files\")\n",
    "\n",
    "print(\"\\nHeader analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff16c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Process individual CSV files and extract target columns\n",
    "\n",
    "def process_single_csv(file_path, header_mapping, add_source_info=True):\n",
    "    \"\"\"Process a single CSV file and extract target columns.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        # Read CSV with encoding handling\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='latin-1')\n",
    "                print(f\"  Used latin-1 encoding\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(file_path, encoding='cp1252')\n",
    "                print(f\"  Used cp1252 encoding\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"  Warning: Empty file\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  Original shape: {df.shape}\")\n",
    "        print(f\"  Original headers: {list(df.columns)}\")\n",
    "        \n",
    "        # Match headers to canonical ones\n",
    "        df_headers = list(df.columns)\n",
    "        matched_canonical_headers, col_indices, final_headers = match_headers_to_canonical(df_headers, header_mapping)\n",
    "        \n",
    "        # Check if we have at least 2 matched headers\n",
    "        if len(matched_canonical_headers) < 2:\n",
    "            print(f\"  Warning: Only found {len(matched_canonical_headers)} relevant headers: {matched_canonical_headers}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"  Matched canonical headers: {matched_canonical_headers}\")\n",
    "        \n",
    "        # Extract the relevant columns\n",
    "        selected_columns = [df.columns[i] for i in col_indices]\n",
    "        extracted_df = df[selected_columns].copy()\n",
    "        \n",
    "        # Rename columns to canonical names\n",
    "        column_rename_map = dict(zip(selected_columns, final_headers))\n",
    "        extracted_df.rename(columns=column_rename_map, inplace=True)\n",
    "        \n",
    "        # Ensure all three canonical columns exist\n",
    "        all_canonical_headers = [\"Learning Outcomes\", \"Deliverables\", \"Assessments\"]\n",
    "        for canonical_header in all_canonical_headers:\n",
    "            if canonical_header not in extracted_df.columns:\n",
    "                extracted_df[canonical_header] = \"\"\n",
    "                \n",
    "        # Reorder columns\n",
    "        extracted_df = extracted_df[all_canonical_headers]\n",
    "        \n",
    "        # Clean the data\n",
    "        cleaned_df = clean_text_data(extracted_df)\n",
    "        \n",
    "        # Remove completely empty rows\n",
    "        cleaned_df = cleaned_df.dropna(how='all')\n",
    "        \n",
    "        # Remove rows where all target columns are empty\n",
    "        mask = (cleaned_df['Learning Outcomes'].str.len() > 0) | \\\n",
    "               (cleaned_df['Deliverables'].str.len() > 0) | \\\n",
    "               (cleaned_df['Assessments'].str.len() > 0)\n",
    "        cleaned_df = cleaned_df[mask]\n",
    "        \n",
    "        # Add source information if requested\n",
    "        if add_source_info:\n",
    "            cleaned_df['Source_File'] = filename\n",
    "            \n",
    "            # Extract course code from filename\n",
    "            course_match = re.match(r'^([A-Z]+[0-9]*)', filename)\n",
    "            if course_match:\n",
    "                cleaned_df['Course_Code'] = course_match.group(1)\n",
    "            else:\n",
    "                cleaned_df['Course_Code'] = ''\n",
    "        \n",
    "        # Reset index\n",
    "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Final shape: {cleaned_df.shape}\")\n",
    "        print(f\"  Successfully processed\")\n",
    "        \n",
    "        return cleaned_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Process all CSV files\n",
    "csv_files = find_csv_files(INPUT_CSV_DIRECTORY)\n",
    "print(f\"Starting to process {len(csv_files)} CSV files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "processed_files = []\n",
    "failed_files = []\n",
    "all_dataframes = []\n",
    "\n",
    "for file_path in csv_files:\n",
    "    cleaned_df = process_single_csv(file_path, HEADER_MAPPING, add_source_info=True)\n",
    "    \n",
    "    if cleaned_df is not None and not cleaned_df.empty:\n",
    "        processed_files.append(file_path)\n",
    "        all_dataframes.append(cleaned_df)\n",
    "        print(f\"  Success: {len(cleaned_df)} rows extracted\")\n",
    "    else:\n",
    "        failed_files.append(file_path)\n",
    "        print(f\"  Failed to process\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Processing Summary:\")\n",
    "print(f\"  Total files: {len(csv_files)}\")\n",
    "print(f\"  Successfully processed: {len(processed_files)}\")\n",
    "print(f\"  Failed: {len(failed_files)}\")\n",
    "print(f\"  Total rows extracted: {sum(len(df) for df in all_dataframes)}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFailed files:\")\n",
    "    for failed_file in failed_files:\n",
    "        print(f\"  - {os.path.basename(failed_file)}\")\n",
    "\n",
    "print(\"\\nIndividual processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062793d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save individual cleaned CSV files\n",
    "\n",
    "def save_individual_files(dataframes_list, original_files_list, output_directory):\n",
    "    \"\"\"Save each cleaned DataFrame as an individual CSV file.\"\"\"\n",
    "    saved_files = []\n",
    "    \n",
    "    print(f\"Saving {len(dataframes_list)} cleaned CSV files...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for df, original_file_path in zip(dataframes_list, original_files_list):\n",
    "        try:\n",
    "            # Create output filename\n",
    "            original_name = Path(original_file_path).stem\n",
    "            cleaned_filename = f\"{original_name}_cleaned.csv\"\n",
    "            output_path = Path(output_directory) / cleaned_filename\n",
    "            \n",
    "            # Save the file\n",
    "            df.to_csv(output_path, index=False)\n",
    "            saved_files.append(str(output_path))\n",
    "            \n",
    "            print(f\"Saved: {cleaned_filename} ({len(df)} rows)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {os.path.basename(original_file_path)}: {str(e)}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Save individual cleaned files\n",
    "if all_dataframes:\n",
    "    saved_individual_files = save_individual_files(all_dataframes, processed_files, OUTPUT_DIRECTORY)\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Individual file saving completed\")\n",
    "    print(f\"Saved {len(saved_individual_files)} files to: {OUTPUT_DIRECTORY}\")\n",
    "    \n",
    "    # Show first few saved files\n",
    "    print(f\"\\nFirst 5 saved files:\")\n",
    "    for i, saved_file in enumerate(saved_individual_files[:5]):\n",
    "        print(f\"  {i+1}. {os.path.basename(saved_file)}\")\n",
    "    \n",
    "    if len(saved_individual_files) > 5:\n",
    "        print(f\"  ... and {len(saved_individual_files) - 5} more files\")\n",
    "        \n",
    "else:\n",
    "    print(\"No dataframes to save - check previous steps for errors\")\n",
    "\n",
    "print(\"\\nIndividual file saving completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create combined dataset (like your original second cell)\n",
    "\n",
    "def create_combined_dataset(dataframes_list, output_directory, filename=\"combined_cleaned_syllabi_data.csv\"):\n",
    "    \"\"\"Combine all cleaned DataFrames into a single CSV file.\"\"\"\n",
    "    \n",
    "    if not dataframes_list:\n",
    "        print(\"No dataframes to combine\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Creating combined dataset from {len(dataframes_list)} files...\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "    \n",
    "    # Sort by source file for better organization\n",
    "    if 'Source_File' in combined_df.columns:\n",
    "        combined_df = combined_df.sort_values('Source_File').reset_index(drop=True)\n",
    "        print(\"Sorted by source file\")\n",
    "    \n",
    "    # Create output path\n",
    "    output_path = Path(output_directory) / filename\n",
    "    \n",
    "    # Save combined file\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Combined dataset saved to: {output_path}\")\n",
    "    print(f\"Total rows in combined dataset: {len(combined_df)}\")\n",
    "    print(f\"Columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    # Show summary by source file\n",
    "    if 'Source_File' in combined_df.columns:\n",
    "        file_counts = combined_df['Source_File'].value_counts()\n",
    "        print(f\"\\nRows per source file:\")\n",
    "        print(f\"  Total unique files: {len(file_counts)}\")\n",
    "        print(f\"  Average rows per file: {file_counts.mean():.1f}\")\n",
    "        print(f\"  Min rows per file: {file_counts.min()}\")\n",
    "        print(f\"  Max rows per file: {file_counts.max()}\")\n",
    "        \n",
    "        print(f\"\\nTop 10 files by row count:\")\n",
    "        for i, (filename, count) in enumerate(file_counts.head(10).items()):\n",
    "            print(f\"  {i+1}. {filename}: {count} rows\")\n",
    "    \n",
    "    # Show data quality summary\n",
    "    print(f\"\\nData quality summary:\")\n",
    "    for col in ['Learning Outcomes', 'Deliverables', 'Assessments']:\n",
    "        if col in combined_df.columns:\n",
    "            non_empty = combined_df[col].str.len() > 0\n",
    "            non_empty_count = non_empty.sum()\n",
    "            percentage = (non_empty_count / len(combined_df)) * 100\n",
    "            print(f\"  {col}: {non_empty_count}/{len(combined_df)} ({percentage:.1f}%) non-empty\")\n",
    "    \n",
    "    return str(output_path)\n",
    "\n",
    "# Create combined dataset\n",
    "if all_dataframes:\n",
    "    combined_file_path = create_combined_dataset(\n",
    "        all_dataframes, \n",
    "        COMBINED_OUTPUT_DIR, \n",
    "        \"combined_cleaned_syllabi_data.csv\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined dataset creation completed\")\n",
    "    print(f\"File location: {combined_file_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No dataframes available for combining\")\n",
    "\n",
    "print(\"\\nCombined dataset ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae19fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Final Summary and Validation\n",
    "\n",
    "def generate_final_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the entire process.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Input summary\n",
    "    print(f\"\\nINPUT:\")\n",
    "    print(f\"  Source directory: {INPUT_CSV_DIRECTORY}\")\n",
    "    csv_files = find_csv_files(INPUT_CSV_DIRECTORY)\n",
    "    print(f\"  Total CSV files found: {len(csv_files)}\")\n",
    "    \n",
    "    # Processing summary\n",
    "    print(f\"\\nPROCESSING RESULTS:\")\n",
    "    print(f\"  Successfully processed: {len(processed_files)}\")\n",
    "    print(f\"  Failed to process: {len(failed_files)}\")\n",
    "    print(f\"  Success rate: {(len(processed_files)/len(csv_files)*100):.1f}%\")\n",
    "    \n",
    "    if all_dataframes:\n",
    "        total_rows = sum(len(df) for df in all_dataframes)\n",
    "        print(f\"  Total rows extracted: {total_rows}\")\n",
    "        print(f\"  Average rows per file: {total_rows/len(all_dataframes):.1f}\")\n",
    "    \n",
    "    # Output summary\n",
    "    print(f\"\\nOUTPUT:\")\n",
    "    print(f\"  Individual cleaned files: {OUTPUT_DIRECTORY}\")\n",
    "    if all_dataframes:\n",
    "        print(f\"    Number of files: {len(all_dataframes)}\")\n",
    "    \n",
    "    print(f\"  Combined dataset: {COMBINED_OUTPUT_DIR}\")\n",
    "    \n",
    "    # Check if combined file exists and get its info\n",
    "    combined_file = Path(COMBINED_OUTPUT_DIR) / \"combined_cleaned_syllabi_data.csv\"\n",
    "    if combined_file.exists():\n",
    "        try:\n",
    "            combined_df = pd.read_csv(combined_file)\n",
    "            print(f\"    Combined file rows: {len(combined_df)}\")\n",
    "            print(f\"    Combined file columns: {list(combined_df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error reading combined file: {e}\")\n",
    "    \n",
    "    # Failed files details\n",
    "    if failed_files:\n",
    "        print(f\"\\nFAILED FILES:\")\n",
    "        for i, failed_file in enumerate(failed_files, 1):\n",
    "            print(f\"  {i}. {os.path.basename(failed_file)}\")\n",
    "    \n",
    "    # Header mapping summary\n",
    "    print(f\"\\nHEADER MAPPING USED:\")\n",
    "    for canonical, variations in HEADER_MAPPING.items():\n",
    "        print(f\"  {canonical}:\")\n",
    "        for variation in variations[:3]:  # Show first 3 variations\n",
    "            print(f\"    - {variation}\")\n",
    "        if len(variations) > 3:\n",
    "            print(f\"    - ... and {len(variations)-3} more variations\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROCESS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def validate_output():\n",
    "    \"\"\"Validate the output files.\"\"\"\n",
    "    \n",
    "    print(\"\\nVALIDATION:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check individual files\n",
    "    individual_files = glob.glob(os.path.join(OUTPUT_DIRECTORY, \"*_cleaned.csv\"))\n",
    "    print(f\"Individual cleaned files: {len(individual_files)} found\")\n",
    "    \n",
    "    # Check combined file\n",
    "    combined_file = Path(COMBINED_OUTPUT_DIR) / \"combined_cleaned_syllabi_data.csv\"\n",
    "    if combined_file.exists():\n",
    "        print(f\"Combined file: EXISTS\")\n",
    "        \n",
    "        # Quick validation of combined file\n",
    "        try:\n",
    "            df_combined = pd.read_csv(combined_file)\n",
    "            print(f\"  Shape: {df_combined.shape}\")\n",
    "            \n",
    "            # Check required columns\n",
    "            required_cols = ['Learning Outcomes', 'Deliverables', 'Assessments']\n",
    "            missing_cols = [col for col in required_cols if col not in df_combined.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"  Warning: Missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                print(f\"  All required columns present: {required_cols}\")\n",
    "            \n",
    "            # Check data completeness\n",
    "            print(f\"  Data completeness:\")\n",
    "            for col in required_cols:\n",
    "                if col in df_combined.columns:\n",
    "                    non_empty = (df_combined[col].astype(str).str.len() > 0) & (df_combined[col] != 'nan')\n",
    "                    count = non_empty.sum()\n",
    "                    percent = (count / len(df_combined)) * 100\n",
    "                    print(f\"    {col}: {count}/{len(df_combined)} ({percent:.1f}%) non-empty\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error validating combined file: {e}\")\n",
    "    else:\n",
    "        print(f\"Combined file: NOT FOUND\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Validation completed\")\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary()\n",
    "\n",
    "# Validate output\n",
    "validate_output()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL STEPS COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nYour cleaned data is ready:\")\n",
    "print(f\"1. Individual files: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"2. Combined file: {COMBINED_OUTPUT_DIR}/combined_cleaned_syllabi_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
