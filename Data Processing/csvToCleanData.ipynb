{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d276f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb40c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(\"packages are imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c94e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "INPUT_CSV_DIRECTORY = \"csv_outputs\"          \n",
    "OUTPUT_DIRECTORY = \"cleaned_csv\"             \n",
    "COMBINED_OUTPUT_DIR = \"combined_csv\"         \n",
    "\n",
    "# define headers and their variations (coming from the docx extraction)\n",
    "HEADER_MAPPING = {\n",
    "    \"Week\": [\n",
    "        \"Week\", \"Week No\", \"Week #\", \"Week Number\", \"Week/Date\"\n",
    "    ],\n",
    "    \"Learning Outcomes\": [\n",
    "        \"Learning Outcomes\", \"Learning Outcome\", \"Learning\\\\nOutcomes\",\n",
    "        \"Learning Outcomes (LO) / Learner and Learning Outcomes (LLO)\",\n",
    "        \"Learning Outcomes\\\\n(At the end of the session, students are expected to:)\",\n",
    "        \"Learning Outcomes\\\\n(At the end of the session, students are expected to :)\",\n",
    "        \"Learning\\\\nOutcomes\", \"Learning \\\\nOutcomes\"\n",
    "    ],\n",
    "    \"Deliverables\": [\n",
    "        \"Deliverables Outcomes\", \"Deliverables/Outcomes\", \"Deliverables\", \n",
    "        \"Deliverables/\\\\nOutcomes\", \"Deliverables\\\\n/ Outcomes\", \"Deliverables/ Outcomes\",\n",
    "        \"Deliverables/\\\\xa0 Outcomes\", \"Deliverables/\\\\xa0\\\\nOutcomes\", \n",
    "        \"Deliverable / Outcomes\", \"Deliverables\\\\n/ Outcomes/Rubrics\"\n",
    "    ],\n",
    "    \"Assessments\": [\n",
    "        \"Assessments\", \"Assessment\", \"Assessment Task\", \"Assessment\\\\n/ Output\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# create output directories \n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(COMBINED_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {INPUT_CSV_DIRECTORY}\")\n",
    "print(f\"Output directory: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"Combined output directory: {COMBINED_OUTPUT_DIR}\")\n",
    "print(\"Configuration completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400707fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_header(text):\n",
    "    \"\"\"\n",
    "    Normalize header text: lowercase, remove spaces, slashes, punctuation.\n",
    "    Same logic as your DOCX extraction script.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
    "\n",
    "def find_csv_files(directory):\n",
    "    \"\"\"Find all CSV files in the specified directory.\"\"\"\n",
    "    csv_pattern = os.path.join(directory, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    return csv_files\n",
    "\n",
    "def match_headers_to_canonical(df_headers, header_mapping):\n",
    "    \"\"\"\n",
    "    Match DataFrame headers to canonical headers using substring matching.\n",
    "    Returns matched canonical headers, column indices, and final header names.\n",
    "    \"\"\"\n",
    "    matched_canonical_headers = []\n",
    "    col_indices = []\n",
    "    final_headers = []\n",
    "\n",
    "    # Normalize input headers\n",
    "    normalized_df_headers = [normalize_header(h) for h in df_headers]\n",
    "\n",
    "    for canonical_header, possible_variations in header_mapping.items():\n",
    "        found_variation_index = -1\n",
    "\n",
    "        # Check if any variation matches as substring\n",
    "        for variation in possible_variations:\n",
    "            normalized_variation = normalize_header(variation)\n",
    "\n",
    "            for j, normalized_table_header in enumerate(normalized_df_headers):\n",
    "                if normalized_variation in normalized_table_header or normalized_table_header in normalized_variation:\n",
    "                    found_variation_index = j\n",
    "                    break\n",
    "\n",
    "            if found_variation_index != -1:\n",
    "                break\n",
    "\n",
    "        if found_variation_index != -1:\n",
    "            col_indices.append(found_variation_index)\n",
    "            final_headers.append(canonical_header)\n",
    "            matched_canonical_headers.append(canonical_header)\n",
    "\n",
    "    return matched_canonical_headers, col_indices, final_headers\n",
    "\n",
    "def clean_text_data(df):\n",
    "    \"\"\"Clean text data removing common DOCX extraction artifacts, including bullet points and more unreadable characters, and formats bullet points as alphabetical lists.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Define common bullet point characters\n",
    "    bullet_chars_pattern = r'[•\\u2022\\uf0b7\\uf0a7\\uf0d8»]'\n",
    "\n",
    "    for col in df_clean.columns:\n",
    "        if col in [\"Learning Outcomes\", \"Deliverables\", \"Assessments\"] and df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "\n",
    "            # Replace unreadable characters first\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\n', ' ', regex=False)\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\xa0', ' ', regex=False) # Non-breaking space\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u2013', '-', regex=False) # En dash\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u2014', '--', regex=False) # Em dash\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u2019', \"'\", regex=False) # Right single quotation mark (apostrophe)\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u201c', '\"', regex=False) # Left double quotation mark\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u201d', '\"', regex=False) # Right double quotation mark\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u2026', '...', regex=False) # Horizontal ellipsis (...)\n",
    "\n",
    "            # Add specific replacements for reported unreadable characters\n",
    "            df_clean[col] = df_clean[col].str.replace('â€¦', '...', regex=False) # Horizontal ellipsis\n",
    "            df_clean[col] = df_clean[col].str.replace('â€“', '-', regex=False) # En dash\n",
    "            df_clean[col] = df_clean[col].str.replace('â€”', '--', regex=False) # Em dash\n",
    "            df_clean[col] = df_clean[col].str.replace('â€™', \"'\", regex=False) # Apostrophe\n",
    "            df_clean[col] = df_clean[col].str.replace('â€œ', '\"', regex=False) # Left double quote\n",
    "            df_clean[col] = df_clean[col].str.replace('â€', '\"', regex=False) # Right double quote\n",
    "\n",
    "            # Replace 'nan' string with empty string\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN'], '', regex=True)\n",
    "\n",
    "            # Normalize text: replace multiple spaces, strip whitespace\n",
    "            df_clean[col] = df_clean[col].str.replace('  +', ' ', regex=True).str.strip()\n",
    "\n",
    "\n",
    "            # --- New: Format bullet points as alphabetical list ---\n",
    "            def format_as_alphabetical_list(text):\n",
    "                if not text:\n",
    "                    return \"\"\n",
    "                # Split the text by bullet points and potential leading whitespace/dashes/newlines\n",
    "                # Use a broad split pattern that includes bullet chars, newlines, and leading dash/space\n",
    "                items = re.split(r'(?:' + bullet_chars_pattern + r'|\\n|\\r|^\\s*[\\-\\–—]\\s*)', text)\n",
    "\n",
    "                # Filter out empty strings resulting from the split\n",
    "                items = [item.strip() for item in items if item.strip()]\n",
    "\n",
    "                if not items:\n",
    "                    return text # Return original text if no list items found\n",
    "\n",
    "                # Format items as a., b., c.\n",
    "                formatted_items = []\n",
    "                for i, item in enumerate(items):\n",
    "                    # Ensure item doesn't already start with a list marker (like a number or letter list)\n",
    "                    if not re.match(r'^\\s*([a-z]\\.|\\d+\\.)\\s', item, re.IGNORECASE):\n",
    "                         formatted_items.append(f\"{chr(ord('a') + i)}. {item}\")\n",
    "                    else:\n",
    "                         formatted_items.append(item) # Keep existing list formatting if present\n",
    "\n",
    "                return ' '.join(formatted_items) # Join with space, or consider newline '\\n'\n",
    "\n",
    "            df_clean[col] = df_clean[col].apply(format_as_alphabetical_list)\n",
    "            # --- End New ---\n",
    "\n",
    "            # Remove leading bullet points/dashes that might remain if not fully captured by split\n",
    "            df_clean[col] = df_clean[col].str.replace(r'^\\s*[' + bullet_chars_pattern + r'\\-—–]\\s*', '', regex=True)\n",
    "\n",
    "            # Final strip after formatting\n",
    "            df_clean[col] = df_clean[col].str.strip()\n",
    "\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "print(\"functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "INPUT_CSV_DIRECTORY = \"csv_outputs\"\n",
    "OUTPUT_DIRECTORY = \"cleaned_csv\"\n",
    "COMBINED_OUTPUT_DIR = \"combined_csv\"\n",
    "\n",
    "def normalize_header(text):\n",
    "    \"\"\"\n",
    "    Normalize header text: lowercase, remove spaces, slashes, punctuation.\n",
    "    Same logic as your DOCX extraction script.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
    "\n",
    "def find_csv_files(directory):\n",
    "    \"\"\"Find all CSV files in the specified directory.\"\"\"\n",
    "    csv_pattern = os.path.join(directory, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    return csv_files\n",
    "\n",
    "def match_headers_to_canonical(df_headers, header_mapping):\n",
    "    \"\"\"\n",
    "    Match DataFrame headers to canonical headers using substring matching.\n",
    "    Returns matched canonical headers, column indices, and final header names.\n",
    "    \"\"\"\n",
    "    matched_canonical_headers = []\n",
    "    col_indices = []\n",
    "    final_headers = []\n",
    "\n",
    "    # Normalize input headers\n",
    "    normalized_df_headers = [normalize_header(h) for h in df_headers]\n",
    "\n",
    "    for canonical_header, possible_variations in header_mapping.items():\n",
    "        found_variation_index = -1\n",
    "\n",
    "        # Check if any variation matches as substring\n",
    "        for variation in possible_variations:\n",
    "            normalized_variation = normalize_header(variation)\n",
    "\n",
    "            for j, normalized_table_header in enumerate(normalized_df_headers):\n",
    "                if normalized_variation in normalized_table_header or normalized_table_header in normalized_variation:\n",
    "                    found_variation_index = j\n",
    "                    break\n",
    "\n",
    "            if found_variation_index != -1:\n",
    "                break\n",
    "\n",
    "        if found_variation_index != -1:\n",
    "            col_indices.append(found_variation_index)\n",
    "            final_headers.append(canonical_header)\n",
    "            matched_canonical_headers.append(canonical_header)\n",
    "\n",
    "    return matched_canonical_headers, col_indices, final_headers\n",
    "\n",
    "def clean_text_data(df):\n",
    "    \"\"\"Clean text data removing common DOCX extraction artifacts, including bullet points and more unreadable characters.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            # Ensure column is string type before applying string methods\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "\n",
    "            # Strip whitespace\n",
    "            df_clean[col] = df_clean[col].str.strip()\n",
    "\n",
    "            # Replace 'nan' string with empty string\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN'], '', regex=True)\n",
    "\n",
    "            # Clean up common formatting issues from DOCX extraction\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\n', ' ', regex=False)\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\xa0', ' ', regex=False) # Non-breaking space\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\u2022', ' ', regex=False) # Bullet point (•)\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\uf0b7', ' ', regex=False) # Another common bullet point character\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\uf0a7', ' ', regex=False) # Yet another bullet point character\n",
    "            df_clean[col] = df_clean[col].str.replace('\\\\uf0d8', ' ', regex=False) # Arrow bullet point\n",
    "\n",
    "            # Add specific replacements for reported unreadable characters\n",
    "            df_clean[col] = df_clean[col].str.replace('â€¦', '...', regex=False) # Horizontal ellipsis\n",
    "            df_clean[col] = df_clean[col].str.replace('â€“', '-', regex=False) # En dash\n",
    "            df_clean[col] = df_clean[col].str.replace('â€”', '--', regex=False) # Em dash\n",
    "\n",
    "            # Remove leading bullet points and other non-alphanumeric characters that might appear at the start\n",
    "            df_clean[col] = df_clean[col].str.replace(r'^\\s*[•\\u2022\\uf0b7\\uf0a7\\uf0d8»\\-—–]\\s*', '', regex=True) # Added dash types and »\n",
    "\n",
    "            # Replace multiple spaces with a single space\n",
    "            df_clean[col] = df_clean[col].str.replace('  +', ' ', regex=True)\n",
    "\n",
    "            # Remove leading/trailing spaces again after cleaning\n",
    "            df_clean[col] = df_clean[col].str.strip()\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def process_single_csv(file_path, header_mapping, add_source_info=True):\n",
    "    \"\"\"Process a single CSV file and extract target columns.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        # read CSV with encoding handling\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='latin-1')\n",
    "                print(f\"  Used latin-1 encoding\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(file_path, encoding='cp1252')\n",
    "                print(f\"  Used cp1252 encoding\")\n",
    "            except Exception as e:\n",
    "                 print(f\"  Error reading CSV with common encodings: {e}\")\n",
    "                 return None\n",
    "\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"  Warning: Empty file\")\n",
    "            return None # return None for empty files\n",
    "\n",
    "        print(f\"  Original shape: {df.shape}\")\n",
    "        print(f\"  Original headers: {list(df.columns)}\")\n",
    "\n",
    "        # match headers to canonical ones\n",
    "        df_headers = list(df.columns)\n",
    "        matched_canonical_headers, col_indices, final_headers = match_headers_to_canonical(df_headers, header_mapping)\n",
    "\n",
    "        print(f\"  Matched canonical headers: {matched_canonical_headers}\")\n",
    "\n",
    "        # extract the relevant columns\n",
    "        # Ensure col_indices are valid for the current dataframe columns\n",
    "        valid_col_indices = [i for i in col_indices if i < len(df.columns)]\n",
    "        selected_columns = [df.columns[i] for i in valid_col_indices]\n",
    "\n",
    "        # Map selected columns to final headers, only for valid indices\n",
    "        column_rename_map = dict(zip(selected_columns, [final_headers[col_indices.index(i)] for i in valid_col_indices]))\n",
    "\n",
    "\n",
    "        # Create a new DataFrame with only the canonical headers, filled with empty strings initially\n",
    "        all_canonical_headers = [\"Week\", \"Learning Outcomes\", \"Deliverables\", \"Assessments\"]\n",
    "        extracted_df = pd.DataFrame(columns=all_canonical_headers)\n",
    "\n",
    "        # Populate the new DataFrame with data from matched columns\n",
    "        if selected_columns:\n",
    "             temp_df = df[selected_columns].copy()\n",
    "             temp_df.rename(columns=column_rename_map, inplace=True)\n",
    "             # Copy data into the correctly structured extracted_df, ensuring columns align\n",
    "             for col in all_canonical_headers:\n",
    "                 if col in temp_df.columns:\n",
    "                     extracted_df[col] = temp_df[col]\n",
    "\n",
    "\n",
    "        # Clean the data\n",
    "        cleaned_df = clean_text_data(extracted_df)\n",
    "\n",
    "        # Remove completely empty rows before specific column cleaning\n",
    "        # Use .str.strip() to consider rows with only whitespace in canonical columns as empty\n",
    "        # Using replace('', pd.NA) and dropna(how='all') is a robust way to remove rows where all *selected* cols are empty after cleaning\n",
    "        cleaned_df = cleaned_df.replace('', pd.NA)\n",
    "        # Only drop rows if ALL canonical columns are NA after cleaning\n",
    "        cleaned_df = cleaned_df.dropna(subset=all_canonical_headers, how='all').fillna('')\n",
    "\n",
    "\n",
    "        # Ensure all target columns are string type before using .str methods\n",
    "        for col in [\"Week\", \"Learning Outcomes\", \"Deliverables\", \"Assessments\"]:\n",
    "            if col in cleaned_df.columns:\n",
    "                    cleaned_df[col] = cleaned_df[col].astype(str)\n",
    "\n",
    "        # Remove only \"nan\" and \"NaN\" strings, but keep all other characters\n",
    "        for col in [\"Week\", \"Learning Outcomes\", \"Deliverables\", \"Assessments\"]:\n",
    "            if col in cleaned_df.columns:\n",
    "                cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN'], '', regex=True)\n",
    "\n",
    "        # For \"Week\" column: replace hyphens/en-dashes/periods/spaces/tabs with comma, extract all numbers, and join as comma-separated\n",
    "        if \"Week\" in cleaned_df.columns:\n",
    "            # Replace various separators with comma\n",
    "            cleaned_df[\"Week\"] = cleaned_df[\"Week\"].str.replace(r'[\\s\\u2013\\u2014\\u2012\\-\\.]', ',', regex=True)\n",
    "            # Extract all numbers\n",
    "            cleaned_df[\"Week\"] = cleaned_df[\"Week\"].apply(lambda x: ','.join(re.findall(r'\\d+', str(x)))) # Ensure x is string\n",
    "            # Remove any accidental double commas and leading/trailing commas\n",
    "            cleaned_df[\"Week\"] = cleaned_df[\"Week\"].str.replace(r',+', ',', regex=True).str.strip(',')\n",
    "\n",
    "            # Ensure the cleaned \"Week\" column only contains numbers and commas\n",
    "            cleaned_df[\"Week\"] = cleaned_df[\"Week\"].apply(lambda x: re.sub(r'[^0-9,]', '', str(x)))\n",
    "\n",
    "        # For other columns (Deliverables, Assessments, Learning Outcomes), ensure hyphens/dashes are just text\n",
    "        for col in [\"Learning Outcomes\", \"Deliverables\", \"Assessments\"]:\n",
    "            if col in cleaned_df.columns:\n",
    "                 # Replace common dash types with standard hyphen\n",
    "                 cleaned_df[col] = cleaned_df[col].str.replace(r'[\\u2013\\u2014\\u2012]', '-', regex=True)\n",
    "                 # Clean leading hyphens or bullet-like characters if not already done by clean_text_data\n",
    "                 cleaned_df[col] = cleaned_df[col].str.replace(r'^\\s*[\\-•»]\\s*', '', regex=True) # Add hyphen to leading removal\n",
    "\n",
    "        # --- Drop columns that are entirely empty after cleaning and row filtering ---\n",
    "        # Check for columns where all values are either empty strings or NaN\n",
    "        cols_to_drop = []\n",
    "        for col in cleaned_df.columns:\n",
    "             # Consider columns with only whitespace as empty for dropping\n",
    "             if cleaned_df[col].astype(str).str.strip().replace('', pd.NA).isnull().all():\n",
    "                 cols_to_drop.append(col)\n",
    "\n",
    "        if cols_to_drop:\n",
    "             print(f\"  Dropping empty columns: {cols_to_drop}\")\n",
    "             cleaned_df = cleaned_df.drop(columns=cols_to_drop)\n",
    "        # --- END NEW ---\n",
    "\n",
    "\n",
    "        # add source information if requested\n",
    "        if add_source_info:\n",
    "            cleaned_df['Source_File'] = filename\n",
    "\n",
    "            # extract course code from filename\n",
    "            course_match = re.match(r'^([A-Z]+[0-9]*)', filename)\n",
    "            if course_match:\n",
    "                cleaned_df['Course_Code'] = course_match.group(1)\n",
    "            else:\n",
    "                cleaned_df['Course_Code'] = ''\n",
    "\n",
    "        # reset index\n",
    "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "\n",
    "        print(f\"  Final shape: {cleaned_df.shape}\")\n",
    "        print(f\"  Successfully processed\")\n",
    "\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "        # return None in case of unexpected errors during processing\n",
    "        return None\n",
    "\n",
    "\n",
    "# process all CSV files\n",
    "csv_files = find_csv_files(INPUT_CSV_DIRECTORY)\n",
    "print(f\"Starting to process {len(csv_files)} CSV files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "processed_files = []\n",
    "failed_files = []\n",
    "all_dataframes = []\n",
    "\n",
    "for file_path in csv_files:\n",
    "    # pass the HEADER_MAPPING from cell 4 to the processing function\n",
    "    cleaned_df = process_single_csv(file_path, HEADER_MAPPING, add_source_info=True)\n",
    "\n",
    "    # check if processing was successful (function did not return None)\n",
    "    if cleaned_df is not None:\n",
    "        processed_files.append(file_path)\n",
    "        # append the DataFrame only if processing was successful and it's not empty\n",
    "        if not cleaned_df.empty:\n",
    "             all_dataframes.append(cleaned_df)\n",
    "             print(f\"  Success: {len(cleaned_df)} rows extracted\")\n",
    "        else:\n",
    "             # file processed successfully but resulted in an empty DataFrame after cleaning\n",
    "             print(f\"  Success: No rows extracted after cleaning\")\n",
    "\n",
    "    else:\n",
    "        # file processing failed (due to empty file or unexpected error)\n",
    "        failed_files.append(file_path)\n",
    "        print(f\"  Failed to process\")\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Processing Summary:\")\n",
    "print(f\"  Total files: {len(csv_files)}\")\n",
    "print(f\"  Successfully processed: {len(processed_files)}\")\n",
    "print(f\"  Failed: {len(failed_files)}\")\n",
    "print(f\"  Success rate: {(len(processed_files)/len(csv_files)*100 if len(csv_files) > 0 else 0):.1f}%\") # Handle division by zero\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFailed files:\")\n",
    "    for failed_file in failed_files:\n",
    "        print(f\"  - {os.path.basename(failed_file)}\")\n",
    "\n",
    "    # a process where in it states what file failed and why\n",
    "    print(\"\\nheaders:\")\n",
    "    for failed_file in failed_files:\n",
    "        try:\n",
    "            # Read only a few lines to get headers without loading the whole potentially problematic file\n",
    "            with open(failed_file, 'r', encoding='utf-8') as f:\n",
    "                 first_line = f.readline().strip()\n",
    "                 headers = first_line.split(',') # Simple split assuming comma delimiter\n",
    "            print(f\"  - {os.path.basename(failed_file)} headers: {headers}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {os.path.basename(failed_file)} error reading headers: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nIndividual processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062793d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_individual_files(dataframes_list, original_files_list, output_directory):\n",
    "    \"\"\"Save each cleaned DataFrame as an individual CSV file.\"\"\"\n",
    "    saved_files = []\n",
    "    \n",
    "    print(f\"Saving {len(dataframes_list)} cleaned CSV files...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for df, original_file_path in zip(dataframes_list, original_files_list):\n",
    "        try:\n",
    "            # Create output filename\n",
    "            original_name = Path(original_file_path).stem\n",
    "            cleaned_filename = f\"{original_name}_cleaned.csv\"\n",
    "            output_path = Path(output_directory) / cleaned_filename\n",
    "            \n",
    "            # Save the file\n",
    "            df.to_csv(output_path, index=False)\n",
    "            saved_files.append(str(output_path))\n",
    "            \n",
    "            print(f\"Saved: {cleaned_filename} ({len(df)} rows)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {os.path.basename(original_file_path)}: {str(e)}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Save individual cleaned files\n",
    "if all_dataframes:\n",
    "    saved_individual_files = save_individual_files(all_dataframes, processed_files, OUTPUT_DIRECTORY)\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Individual file saving completed\")\n",
    "    print(f\"Saved {len(saved_individual_files)} files to: {OUTPUT_DIRECTORY}\")\n",
    "    \n",
    "    # Show first few saved files\n",
    "    print(f\"\\nFirst 5 saved files:\")\n",
    "    for i, saved_file in enumerate(saved_individual_files[:5]):\n",
    "        print(f\"  {i+1}. {os.path.basename(saved_file)}\")\n",
    "    \n",
    "    if len(saved_individual_files) > 5:\n",
    "        print(f\"  ... and {len(saved_individual_files) - 5} more files\")\n",
    "        \n",
    "else:\n",
    "    print(\"No dataframes to save - check previous steps for errors\")\n",
    "\n",
    "print(\"\\nIndividual file saving completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset(dataframes_list, output_directory, filename=\"combined_cleaned_syllabi_data.csv\"):\n",
    "    \"\"\"Combine all cleaned DataFrames into a single CSV file.\"\"\"\n",
    "    \n",
    "    if not dataframes_list:\n",
    "        print(\"No dataframes to combine\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Creating combined dataset from {len(dataframes_list)} files...\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "    \n",
    "    # Sort by source file for better organization\n",
    "    if 'Source_File' in combined_df.columns:\n",
    "        combined_df = combined_df.sort_values('Source_File').reset_index(drop=True)\n",
    "        print(\"Sorted by source file\")\n",
    "    \n",
    "    # Create output path\n",
    "    output_path = Path(output_directory) / filename\n",
    "    \n",
    "    # Save combined file\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Combined dataset saved to: {output_path}\")\n",
    "    print(f\"Total rows in combined dataset: {len(combined_df)}\")\n",
    "    print(f\"Columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    # Show summary by source file\n",
    "    if 'Source_File' in combined_df.columns:\n",
    "        file_counts = combined_df['Source_File'].value_counts()\n",
    "        print(f\"\\nRows per source file:\")\n",
    "        print(f\"  Total unique files: {len(file_counts)}\")\n",
    "        print(f\"  Average rows per file: {file_counts.mean():.1f}\")\n",
    "        print(f\"  Min rows per file: {file_counts.min()}\")\n",
    "        print(f\"  Max rows per file: {file_counts.max()}\")\n",
    "        \n",
    "        print(f\"\\nTop 10 files by row count:\")\n",
    "        for i, (filename, count) in enumerate(file_counts.head(10).items()):\n",
    "            print(f\"  {i+1}. {filename}: {count} rows\")\n",
    "    \n",
    "    # Show data quality summary\n",
    "    print(f\"\\nData quality summary:\")\n",
    "    for col in ['Learning Outcomes', 'Deliverables', 'Assessments']:\n",
    "        if col in combined_df.columns:\n",
    "            non_empty = combined_df[col].str.len() > 0\n",
    "            non_empty_count = non_empty.sum()\n",
    "            percentage = (non_empty_count / len(combined_df)) * 100\n",
    "            print(f\"  {col}: {non_empty_count}/{len(combined_df)} ({percentage:.1f}%) non-empty\")\n",
    "    \n",
    "    return str(output_path)\n",
    "\n",
    "# Create combined dataset\n",
    "if all_dataframes:\n",
    "    combined_file_path = create_combined_dataset(\n",
    "        all_dataframes, \n",
    "        COMBINED_OUTPUT_DIR, \n",
    "        \"cleanbatch_syllabi_data.csv\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined dataset creation completed\")\n",
    "    print(f\"File location: {combined_file_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No dataframes available for combining\")\n",
    "\n",
    "print(\"\\nCombined dataset ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae19fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the entire process.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Input summary\n",
    "    print(f\"\\nINPUT:\")\n",
    "    print(f\"  Source directory: {INPUT_CSV_DIRECTORY}\")\n",
    "    csv_files = find_csv_files(INPUT_CSV_DIRECTORY)\n",
    "    print(f\"  Total CSV files found: {len(csv_files)}\")\n",
    "    \n",
    "    # Processing summary\n",
    "    print(f\"\\nPROCESSING RESULTS:\")\n",
    "    print(f\"  Successfully processed: {len(processed_files)}\")\n",
    "    print(f\"  Failed to process: {len(failed_files)}\")\n",
    "    print(f\"  Success rate: {(len(processed_files)/len(csv_files)*100):.1f}%\")\n",
    "    \n",
    "    if all_dataframes:\n",
    "        total_rows = sum(len(df) for df in all_dataframes)\n",
    "        print(f\"  Total rows extracted: {total_rows}\")\n",
    "        print(f\"  Average rows per file: {total_rows/len(all_dataframes):.1f}\")\n",
    "    \n",
    "    # Output summary\n",
    "    print(f\"\\nOUTPUT:\")\n",
    "    print(f\"  Individual cleaned files: {OUTPUT_DIRECTORY}\")\n",
    "    if all_dataframes:\n",
    "        print(f\"    Number of files: {len(all_dataframes)}\")\n",
    "    \n",
    "    print(f\"  Combined dataset: {COMBINED_OUTPUT_DIR}\")\n",
    "    \n",
    "    # Check if combined file exists and get its info\n",
    "    combined_file = Path(COMBINED_OUTPUT_DIR) / \"combined_cleaned_syllabi_data.csv\"\n",
    "    if combined_file.exists():\n",
    "        try:\n",
    "            combined_df = pd.read_csv(combined_file)\n",
    "            print(f\"    Combined file rows: {len(combined_df)}\")\n",
    "            print(f\"    Combined file columns: {list(combined_df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error reading combined file: {e}\")\n",
    "    \n",
    "    # Failed files details\n",
    "    if failed_files:\n",
    "        print(f\"\\nFAILED FILES:\")\n",
    "        for i, failed_file in enumerate(failed_files, 1):\n",
    "            print(f\"  {i}. {os.path.basename(failed_file)}\")\n",
    "    \n",
    "    # Header mapping summary\n",
    "    print(f\"\\nHEADER MAPPING USED:\")\n",
    "    for canonical, variations in HEADER_MAPPING.items():\n",
    "        print(f\"  {canonical}:\")\n",
    "        for variation in variations[:3]:  # Show first 3 variations\n",
    "            print(f\"    - {variation}\")\n",
    "        if len(variations) > 3:\n",
    "            print(f\"    - ... and {len(variations)-3} more variations\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROCESS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def validate_output():\n",
    "    \"\"\"Validate the output files.\"\"\"\n",
    "    \n",
    "    print(\"\\nVALIDATION:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check individual files\n",
    "    individual_files = glob.glob(os.path.join(OUTPUT_DIRECTORY, \"*_cleaned.csv\"))\n",
    "    print(f\"Individual cleaned files: {len(individual_files)} found\")\n",
    "    \n",
    "    # Check combined file\n",
    "    combined_file = Path(COMBINED_OUTPUT_DIR) / \"combined_cleaned_syllabi_data.csv\"\n",
    "    if combined_file.exists():\n",
    "        print(f\"Combined file: EXISTS\")\n",
    "        \n",
    "        # Quick validation of combined file\n",
    "        try:\n",
    "            df_combined = pd.read_csv(combined_file)\n",
    "            print(f\"  Shape: {df_combined.shape}\")\n",
    "            \n",
    "            # Check required columns\n",
    "            required_cols = ['Learning Outcomes', 'Deliverables', 'Assessments']\n",
    "            missing_cols = [col for col in required_cols if col not in df_combined.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"  Warning: Missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                print(f\"  All required columns present: {required_cols}\")\n",
    "            \n",
    "            # Check data completeness\n",
    "            print(f\"  Data completeness:\")\n",
    "            for col in required_cols:\n",
    "                if col in df_combined.columns:\n",
    "                    non_empty = (df_combined[col].astype(str).str.len() > 0) & (df_combined[col] != 'nan')\n",
    "                    count = non_empty.sum()\n",
    "                    percent = (count / len(df_combined)) * 100\n",
    "                    print(f\"    {col}: {count}/{len(df_combined)} ({percent:.1f}%) non-empty\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error validating combined file: {e}\")\n",
    "    else:\n",
    "        print(f\"Combined file: NOT FOUND\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Validation completed\")\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary()\n",
    "\n",
    "# Validate output\n",
    "validate_output()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL STEPS COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nYour cleaned data is ready:\")\n",
    "print(f\"1. Individual files: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"2. Combined file: {COMBINED_OUTPUT_DIR}/cleanbatch_syllabi_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
