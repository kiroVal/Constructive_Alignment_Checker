{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0euQbwrg5p_n",
        "outputId": "4cd01d69-4236-4e2f-98f5-80184adeb0c1"
      },
      "outputs": [],
      "source": [
        "%pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pI_BzRc2P3Q",
        "outputId": "75b75e26-80bd-425e-948e-bb75b99e10a6"
      },
      "outputs": [],
      "source": [
        "from docx import Document\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "# === SETTINGS ===\n",
        "input_folder = \"docx_folder\"   # source folder with Word files\n",
        "output_folder = \"csv_outputs\" # destination folder for CSVs\n",
        "# Define canonical headers and their possible variations in source documents\n",
        "header_mapping = {\n",
        "    # Added more variations for Week to capture numbered lists and common patterns\n",
        "    \"Week\": [\"Week\", \"Week No\", \"Week #\", \"Week Number\", \"Week/Date\", \"Weeks\", \"WK\"],\n",
        "    \"Learning Outcomes\": [\"Learning Outcomes\", \"Learning Outcome\", \"Learning\\nOutcomes\", \"Learning Outcomes (LO) / Learner and Learning Outcomes (LLO)\", \"Learning Outcomes\\n(At the end of the session, students are expected to:)\", \"Learning Outcomes\\n(At the end of the session, students are expected to :)\", \"Learning\\nOutcomes\", \"Learning \\nOutcomes\"],\n",
        "    # Added more variations for Deliverables Outcomes\n",
        "    \"Deliverables\": [\"Deliverables Outcomes\", \"Deliverables/Outcomes\", \"Deliverables\", \"Deliverables/\\nOutcomes\", \"Deliverables\\n/ Outcomes\", \"Deliverables/ Outcomes\", \"Deliverables/\\xa0 Outcomes\", \"Deliverables/\\xa0\\nOutcomes\", \"Deliverable / Outcomes\", \"Deliverables\\n/ Outcomes/Rubrics\"],\n",
        "    # Added more variations for Assessments\n",
        "    \"Assessments\": [\"Assessments\", \"Assessment\", \"Assessment Task\", \"Assessment\\n/ Output\"]\n",
        "}\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# === FUNCTIONS ===\n",
        "def normalize_header(text):\n",
        "    \"\"\"\n",
        "    Normalize header text: lowercase, remove spaces, slashes, punctuation.\n",
        "    E.g., 'Deliverables/ Outcomes' -> 'deliverablesoutcomes'\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
        "\n",
        "# Normalize header mapping for easier lookup\n",
        "normalized_header_mapping = {\n",
        "    normalize_header(k): [normalize_header(v) for v in values]\n",
        "    for k, values in header_mapping.items()\n",
        "}\n",
        "\n",
        "# Get a set of all possible normalized variations we are looking for\n",
        "all_normalized_variations = set()\n",
        "for variations in normalized_header_mapping.values():\n",
        "    all_normalized_variations.update(variations)\n",
        "\n",
        "\n",
        "# === MAIN LOOP ===\n",
        "for file in glob.glob(os.path.join(input_folder, \"*.docx\")):\n",
        "    doc = Document(file)\n",
        "    file_found_relevant_table = False # Track if any relevant table was found in the file\n",
        "\n",
        "    print(f\"\\nProcessing DOCX file: {os.path.basename(file)}\")\n",
        "\n",
        "    for i, table in enumerate(doc.tables):\n",
        "        # Check if the table has at least one row (the header row)\n",
        "        if len(table.rows) > 0:\n",
        "            headers = [cell.text.strip() for cell in table.rows[0].cells]\n",
        "            normalized_headers = [normalize_header(h) for h in headers]\n",
        "\n",
        "            # Check if a significant number of wanted headers are present as substrings in the first row headers\n",
        "            matched_canonical_headers = []\n",
        "            col_indices = []\n",
        "            final_headers = []\n",
        "\n",
        "            for canonical_header, possible_variations in header_mapping.items():\n",
        "                normalized_canonical_header = normalize_header(canonical_header)\n",
        "                found_variation_index = -1\n",
        "                # Check if any of the normalized variations are substrings of the normalized headers in the table\n",
        "                for variation in possible_variations:\n",
        "                    normalized_variation = normalize_header(variation)\n",
        "                    for j, normalized_table_header in enumerate(normalized_headers):\n",
        "                        # Use exact match or significant substring match\n",
        "                        if normalized_variation == normalized_table_header or (len(normalized_variation) > 3 and normalized_variation in normalized_table_header) or (len(normalized_table_header) > 3 and normalized_table_header in normalized_variation):\n",
        "                            found_variation_index = j # Found a match at this column index\n",
        "                            break # Found a match for this canonical header, move to the next canonical header\n",
        "                    if found_variation_index != -1:\n",
        "                        break # Found a match for this canonical header variation\n",
        "\n",
        "                if found_variation_index != -1:\n",
        "                    col_indices.append(found_variation_index)\n",
        "                    final_headers.append(canonical_header) # Use the canonical header in the final DataFrame\n",
        "                    matched_canonical_headers.append(canonical_header)\n",
        "\n",
        "\n",
        "            # If at least 2 canonical headers were matched, consider this a relevant table\n",
        "            if len(matched_canonical_headers) >= 2:\n",
        "                print(f\"  Found potentially relevant Table {i+1}.\")\n",
        "                print(f\"    Headers in Table {i+1} (first row): {headers}\")\n",
        "                print(f\"    Matched canonical headers: {matched_canonical_headers}\")\n",
        "\n",
        "\n",
        "                 # Extract data if wanted columns are found (should be true based on the >=2 check)\n",
        "                if col_indices:\n",
        "                    data = []\n",
        "                    for row_idx, row in enumerate(table.rows[1:]): # Skip header row, iterate with index\n",
        "                        row_data = []\n",
        "                        for j, header in zip(col_indices, final_headers):\n",
        "                            cell_text = \"\"\n",
        "                            if j < len(row.cells): # Check if cell index is valid\n",
        "                                cell_text = row.cells[j].text.strip()\n",
        "\n",
        "                            # --- Week Number Extraction Debugging and Improvement in DOCX processing ---\n",
        "                            if header == \"Week\":\n",
        "                                print(f\"      Table {i+1}, Row {row_idx+2} (Data Row), Week Cell Raw Text: '{cell_text}'\") # Debugging print\n",
        "\n",
        "                                # Attempt to find sequences of digits that look like week numbers.\n",
        "                                # Look for digits potentially followed by common separators or at word boundaries.\n",
        "                                # This regex is made more flexible to capture numbers in various contexts.\n",
        "                                numbers = re.findall(r'\\b\\d+\\b(?:[-\\.\\,]\\s*\\b\\d+\\b)*', cell_text) # Find numbers, potentially followed by range/list separators\n",
        "\n",
        "                                # If no multi-number patterns found, try to find single numbers more broadly\n",
        "                                if not numbers:\n",
        "                                    numbers = re.findall(r'\\d+', cell_text) # Just find any digits if specific patterns fail\n",
        "\n",
        "\n",
        "                                extracted_week_str = ','.join(sorted(list(set(numbers)), key=lambda x: int(x) if x.isdigit() else 0)) if numbers else '' # Sort unique numbers (handle non-digit sort key for safety)\n",
        "\n",
        "                                print(f\"      Table {i+1}, Row {row_idx+2}, Extracted Week Numbers: '{extracted_week_str}'\") # Debugging print\n",
        "                                cell_text = extracted_week_str # Use the extracted numbers for the Week column\n",
        "\n",
        "\n",
        "                             # --- Handle Hyphens and special characters in other columns (like Deliverables) ---\n",
        "                             # Replace newlines and multiple spaces\n",
        "                            cell_text = cell_text.replace('\\n', ' ').replace('\\r', ' ') # Replace newlines in cell text\n",
        "                            cell_text = re.sub(r'\\s+', ' ', cell_text).strip() # Replace multiple spaces with single space and strip whitespace\n",
        "                            # Explicitly handle common unreadable characters that might appear\n",
        "                            cell_text = cell_text.replace('â€¦', '...')\n",
        "                            cell_text = cell_text.replace('â€“', '-')\n",
        "                            cell_text = cell_text.replace('â€”', '--')\n",
        "\n",
        "\n",
        "                            row_data.append(cell_text) # Add cleaned cell text to row data\n",
        "                        data.append(row_data)\n",
        "\n",
        "                    # Create DataFrame and save to CSV\n",
        "                    df = pd.DataFrame(data, columns=final_headers)\n",
        "\n",
        "                    # Ensure all four canonical columns exist, even if they weren't in the original table\n",
        "                    all_canonical_headers = [\"Week\", \"Learning Outcomes\", \"Deliverables\", \"Assessments\"]\n",
        "                    for canonical_header in all_canonical_headers:\n",
        "                        if canonical_header not in df.columns:\n",
        "                            df[canonical_header] = \"\" # Add missing column with empty strings\n",
        "\n",
        "                    # Reorder columns to the desired canonical order\n",
        "                    df = df[all_canonical_headers]\n",
        "\n",
        "                    # Remove rows where all four canonical columns are empty\n",
        "                    mask = (df['Week'].astype(str).str.len() > 0) | \\\n",
        "                           (df['Learning Outcomes'].astype(str).str.len() > 0) | \\\n",
        "                           (df['Deliverables'].astype(str).str.len() > 0) | \\\n",
        "                           (df['Assessments'].astype(str).str.len() > 0)\n",
        "                    df = df[mask]\n",
        "\n",
        "\n",
        "                    output_filename = os.path.join(output_folder, os.path.splitext(os.path.basename(file))[0] + \".csv\")\n",
        "                    df.to_csv(output_filename, index=False) # pandas to_csv handles quoting fields with commas/special chars\n",
        "                    print(f\"    Successfully extracted data from Table {i+1} to {os.path.basename(output_filename)}\")\n",
        "                    file_found_relevant_table = True\n",
        "                    # Don't break, continue checking other tables in the same file in case there are multiple relevant tables\n",
        "\n",
        "\n",
        "    if not file_found_relevant_table:\n",
        "        print(f\"\\nNo relevant table with at least 2 of the main headers {list(header_mapping.keys())} found in {os.path.basename(file)}\")\n",
        "\n",
        "\n",
        "print(\"\\nProcessing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "851845bf",
        "outputId": "070e07ca-b975-40fa-e036-8a9d78634b56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Print the current working directory\n",
        "cwd = os.getcwd()\n",
        "print(f\"Current working directory: {cwd}\")\n",
        "\n",
        "# Use relative path for the source folder\n",
        "source_csv_folder = \"csv_outputs\"\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(source_csv_folder):\n",
        "    print(f\"The folder '{source_csv_folder}' does not exist.\")\n",
        "else:\n",
        "    # List all files in the folder for debugging\n",
        "    all_files = os.listdir(source_csv_folder)\n",
        "    print(f\"Files in '{source_csv_folder}': {all_files}\")\n",
        "\n",
        "    # Get a list of all CSV files in the source folder\n",
        "    csv_files = glob.glob(os.path.join(source_csv_folder, \"*.csv\"))\n",
        "    print(f\"CSV files found: {csv_files}\")\n",
        "\n",
        "    # Change the output folder for the combined CSV\n",
        "    output_folder = \"combined_csv\"\n",
        "    combined_csv_filename = \"combined_syllabi_data.csv\"\n",
        "    combined_csv_filepath = os.path.join(output_folder, combined_csv_filename)\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Check if there are any CSV files to combine\n",
        "    if not csv_files:\n",
        "        print(f\"No CSV files found in {source_csv_folder} to combine.\")\n",
        "    else:\n",
        "        # Create an empty list to store DataFrames\n",
        "        df_list = []\n",
        "\n",
        "        # Read each CSV file into a DataFrame and append to the list\n",
        "        for file in csv_files:\n",
        "            try:\n",
        "                df = pd.read_csv(file, dtype={'Week': str})\n",
        "                df_list.append(df)\n",
        "                print(f\"Read {os.path.basename(file)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {os.path.basename(file)}: {e}\")\n",
        "\n",
        "        # Concatenate all DataFrames into a single DataFrame\n",
        "        if df_list:\n",
        "            combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "            # Save the combined DataFrame to a new CSV file in the specified output folder\n",
        "            combined_df.to_csv(combined_csv_filepath, index=False)\n",
        "\n",
        "            print(f\"\\nSuccessfully combined {len(csv_files)} CSV files into {combined_csv_filename}\")\n",
        "            print(f\"Combined CSV saved to: {combined_csv_filepath}\")\n",
        "        else:\n",
        "            print(\"No DataFrames were successfully read to combine.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
