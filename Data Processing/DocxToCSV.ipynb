{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec51834",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the input folder containing the .docx files\n",
    "input_folder = 'C:\\\\REFACTOR\\\\docx_input'\n",
    "\n",
    "# Define the output folder where the cleaned CSVs will be saved\n",
    "output_folder = 'C:\\\\REFACTOR\\\\docx_output'\n",
    "\n",
    "# ensure that the folders exists\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"Input folder: {input_folder}\")\n",
    "print(f\"Output folder: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Ensure input_folder is defined from the setup step\n",
    "# input_folder = './input_docx' # Assuming this is defined in a previous cell\n",
    "\n",
    "docx_files = glob.glob(os.path.join(input_folder, '*.docx'))\n",
    "print(f\"Found {len(docx_files)} DOCX files:\")\n",
    "for docx_file in docx_files:\n",
    "    print(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325130d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Nothing to process.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        # The rest of the processing for each file will go here in subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Nothing to process.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        try:\n",
    "            document = Document(docx_file)\n",
    "            print(f\"Successfully opened {docx_file}\")\n",
    "            # The rest of the processing for the document will go here\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening {docx_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0444b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Assuming docx_files is populated from a previous step\n",
    "docx_files = glob.glob(os.path.join(input_folder, '*.docx')) # Example from previous step\n",
    "\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Cannot process tables.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        try:\n",
    "            document = Document(docx_file)\n",
    "            course_outline_found = False\n",
    "\n",
    "            # Look for \"COURSE OUTLINE\" in paragraphs\n",
    "            for paragraph in document.paragraphs:\n",
    "                if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                    course_outline_found = True\n",
    "                    print(f\"Found 'COURSE OUTLINE' section in {docx_file}\")\n",
    "                    break # Stop searching for the section once found\n",
    "\n",
    "            if not course_outline_found:\n",
    "                print(f\"'COURSE OUTLINE' section not found in {docx_file}\")\n",
    "            # The rest of the processing (finding tables) will go here in subsequent steps\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {docx_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea520e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Need re for cleaning in a later step, but including it here for the overall process flow\n",
    "from docx import Document\n",
    "\n",
    "# Assuming docx_files is populated from a previous step\n",
    "# docx_files = glob.glob(os.path.join(input_folder, '*.docx')) # Example from previous step\n",
    "\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Cannot process tables.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        try:\n",
    "            document = Document(docx_file)\n",
    "            course_outline_found = False\n",
    "            # Updated target names to include variations and partial matches (excluding CO aligned to)\n",
    "            target_table_names = [\"Learning Outcomes\", \"Deliverables\", \"Assessment\", \"Instructional Materials\"]\n",
    "            extracted_tables = {} # Dictionary to store extracted tables by name\n",
    "\n",
    "            # First, look for \"COURSE OUTLINE\" in paragraphs\n",
    "            for paragraph in document.paragraphs:\n",
    "                if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                    course_outline_found = True\n",
    "                    print(f\"Found 'COURSE OUTLINE' section in {docx_file}\")\n",
    "                    break\n",
    "\n",
    "            # If \"COURSE OUTLINE\" is found, look for target tables\n",
    "            if course_outline_found:\n",
    "                print(f\"  Examining {len(document.tables)} tables in document...\")\n",
    "                for i, table in enumerate(document.tables):\n",
    "                    if table.rows and table.rows[0].cells:\n",
    "                        # Get all text from the first row to analyze the table structure\n",
    "                        first_row_all_text = []\n",
    "                        for cell in table.rows[0].cells:\n",
    "                            first_row_all_text.append(cell.text.strip())\n",
    "                        \n",
    "                        full_first_row = \" | \".join(first_row_all_text)\n",
    "                        print(f\"    Table {i+1} first row: {full_first_row}\")\n",
    "                        \n",
    "                        # Check if this looks like the main course outline table (contains multiple target columns)\n",
    "                        targets_found_in_table = []\n",
    "                        for target_name in target_table_names:\n",
    "                            for cell_text in first_row_all_text:\n",
    "                                if target_name.lower() in cell_text.lower():\n",
    "                                    targets_found_in_table.append(target_name)\n",
    "                                    break\n",
    "                        \n",
    "                        if len(targets_found_in_table) >= 2:  # If table contains 2+ target columns\n",
    "                            print(f\"      *** MAIN TABLE FOUND: Contains {targets_found_in_table}\")\n",
    "                            extracted_tables[\"Main_Course_Table\"] = table\n",
    "                            # Also store individual references for compatibility\n",
    "                            for target in targets_found_in_table:\n",
    "                                extracted_tables[target] = table\n",
    "\n",
    "            if not course_outline_found:\n",
    "                print(f\"'COURSE OUTLINE' section not found in {docx_file}\")\n",
    "            elif not extracted_tables:\n",
    "                 print(f\"No target tables found in the 'COURSE OUTLINE' section of {docx_file}\")\n",
    "            else:\n",
    "                print(f\"Extracted {len(extracted_tables)} target table references from {docx_file}.\")\n",
    "                print(f\"  Found tables: {list(extracted_tables.keys())}\")\n",
    "                # The actual processing of table data (rows, cells, cleaning) will happen in subsequent steps\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {docx_file}: {e}\")\n",
    "        \n",
    "        # Stop after processing the first file for debugging\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51828f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Assuming docx_files is populated from a previous step\n",
    "# Assuming the loop to iterate through docx_files is in place\n",
    "\n",
    "# For demonstration purposes, if no files were found in previous steps:\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Cannot process tables.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        try:\n",
    "            document = Document(docx_file)\n",
    "            course_outline_found = False\n",
    "            # List to store tables found after \"COURSE OUTLINE\"\n",
    "            tables_after_course_outline = []\n",
    "\n",
    "            # First, look for \"COURSE OUTLINE\" in paragraphs\n",
    "            for paragraph in document.paragraphs:\n",
    "                if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                    course_outline_found = True\n",
    "                    print(f\"Found 'COURSE OUTLINE' section in {docx_file}\")\n",
    "                    break\n",
    "\n",
    "            # If \"COURSE OUTLINE\" is found, collect all tables\n",
    "            if course_outline_found:\n",
    "                for table in document.tables:\n",
    "                    # Add all tables to the list (we'll filter them later if needed)\n",
    "                    tables_after_course_outline.append(table)\n",
    "                    print(f\"  Found a table after 'COURSE OUTLINE'. Total tables found so far: {len(tables_after_course_outline)}\")\n",
    "\n",
    "            if not course_outline_found:\n",
    "                print(f\"'COURSE OUTLINE' section not found in {docx_file}\")\n",
    "            elif not tables_after_course_outline:\n",
    "                 print(f\"No tables found after the 'COURSE OUTLINE' section in {docx_file}\")\n",
    "            else:\n",
    "                print(f\"Found {len(tables_after_course_outline)} tables after 'COURSE OUTLINE' section in {docx_file}.\")\n",
    "                # The list tables_after_course_outline now contains all tables\n",
    "                # after the \"COURSE OUTLINE\" section for the current file.\n",
    "                # These tables will be processed in subsequent steps.\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {docx_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "\n",
    "# Assuming docx_files is populated from a previous step\n",
    "# Assuming the loop to iterate through docx_files is in place\n",
    "# Assuming the code to find \"COURSE OUTLINE\" and identify extracted_tables is in place\n",
    "\n",
    "# For demonstration purposes, if no files were found in previous steps:\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Cannot process tables.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        try:\n",
    "            document = Document(docx_file) # Assuming Document is available from a previous import\n",
    "            course_outline_found = False\n",
    "            target_table_names = [\"Learning Outcomes\", \"Deliverables Outcomes\", \"Assessment\"]\n",
    "            extracted_tables = {}\n",
    "\n",
    "            # --- Locate \"COURSE OUTLINE\" section and Identify target tables ---\n",
    "            # First, look for \"COURSE OUTLINE\" in paragraphs\n",
    "            for paragraph in document.paragraphs:\n",
    "                if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                    course_outline_found = True\n",
    "                    # print(f\"Found 'COURSE OUTLINE' section in {os.path.basename(docx_file)}\") # Uncomment for debugging\n",
    "                    break\n",
    "\n",
    "            # If \"COURSE OUTLINE\" is found, look for target tables\n",
    "            if course_outline_found:\n",
    "                for table in document.tables:\n",
    "                    # Check if the first cell of the table contains one of the target names\n",
    "                    if table.rows and table.rows[0].cells:\n",
    "                        first_cell_text = table.rows[0].cells[0].text.strip()\n",
    "                        if first_cell_text in target_table_names:\n",
    "                            if first_cell_text not in extracted_tables: # Avoid processing the same table if it appears multiple times\n",
    "                                table_name = first_cell_text\n",
    "                                # print(f\"  Found target table: {table_name}\") # Uncomment for debugging\n",
    "                                extracted_tables[table_name] = table\n",
    "\n",
    "            if not course_outline_found:\n",
    "                print(f\"'COURSE OUTLINE' section not found in {os.path.basename(docx_file)}\")\n",
    "            elif not extracted_tables:\n",
    "                 print(f\"No target tables found in the 'COURSE OUTLINE' section of {os.path.basename(docx_file)}\")\n",
    "            else:\n",
    "                # print(f\"Extracted {len(extracted_tables)} target tables from {os.path.basename(docx_file)}.\") # Uncomment for debugging\n",
    "\n",
    "                # --- Iterate through table data and Clean data with regex (Current Subtask) ---\n",
    "                print(\"  Processing table data and applying regex cleaning:\")\n",
    "                for table_name, table in extracted_tables.items():\n",
    "                    print(f\"    Processing table: {table_name}\")\n",
    "                    for i, row in enumerate(table.rows):\n",
    "                         # Optional: Skip header row if needed, but for raw cell extraction, process all rows\n",
    "                         # if i == 0:\n",
    "                         #     continue\n",
    "\n",
    "                         for cell in row.cells:\n",
    "                            # Get the text content of the cell\n",
    "                            cell_text = cell.text\n",
    "\n",
    "                            # Use regex to replace bullet point characters with dashes\n",
    "                            cleaned_text_bullets = re.sub(r'[\\u2022\\u2023\\u2043\\u00B7]', '-', cell_text)\n",
    "\n",
    "                            # Use regex to replace other special characters with commas\n",
    "                            # This pattern keeps letters, numbers, whitespace, periods, commas, and hyphens\n",
    "                            cleaned_text_final = re.sub(r'[^a-zA-Z0-9\\s.,-]', ',', cleaned_text_bullets)\n",
    "\n",
    "                            # For this subtask, print the cleaned text\n",
    "                            # In the next step, this cleaned_text_final will be stored.\n",
    "                            # print(f\"      Original: '{cell_text[:50]}...'\") # Print first 50 chars for brevity\n",
    "                            # print(f\"      Cleaned:  '{cleaned_text_final[:50]}...'\") # Print first 50 chars for brevity\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(docx_file)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "# Assuming docx_files is populated from a previous step\n",
    "# Assuming the loop to iterate through docx_files is in place\n",
    "# Assuming the code to find \"COURSE OUTLINE\" and identify extracted_tables is in place\n",
    "\n",
    "# Initialize a list to store the combined cleaned data from all files and tables\n",
    "all_files_combined_data = []\n",
    "\n",
    "# For demonstration purposes, if no files were found in previous steps:\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Cannot process tables.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        current_file_extracted_data = [] # Initialize list to hold data for the current file\n",
    "        try:\n",
    "            # Assuming Document is available from a previous import\n",
    "            document = Document(docx_file)\n",
    "            course_outline_found = False\n",
    "            target_table_names = [\"Learning Outcomes\", \"Deliverables\", \"Assessment\", \"Instructional Materials\"]\n",
    "            extracted_tables = {}\n",
    "\n",
    "            # --- Locate \"COURSE OUTLINE\" section and Identify target tables ---\n",
    "            # First, look for \"COURSE OUTLINE\" in paragraphs\n",
    "            for paragraph in document.paragraphs:\n",
    "                if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                    course_outline_found = True\n",
    "                    break\n",
    "\n",
    "            # If \"COURSE OUTLINE\" is found, look for the main course table\n",
    "            if course_outline_found:\n",
    "                for table in document.tables:\n",
    "                    if table.rows and table.rows[0].cells:\n",
    "                        # Get all text from the first row to analyze the table structure\n",
    "                        first_row_all_text = []\n",
    "                        for cell in table.rows[0].cells:\n",
    "                            first_row_all_text.append(cell.text.strip())\n",
    "                        \n",
    "                        # Check if this looks like the main course outline table (contains multiple target columns)\n",
    "                        targets_found_in_table = []\n",
    "                        for target_name in target_table_names:\n",
    "                            for cell_text in first_row_all_text:\n",
    "                                if target_name.lower() in cell_text.lower():\n",
    "                                    targets_found_in_table.append(target_name)\n",
    "                                    break\n",
    "                        \n",
    "                        if len(targets_found_in_table) >= 2:  # If table contains 2+ target columns\n",
    "                            extracted_tables[\"Main_Course_Table\"] = table\n",
    "                            break  # Found the main table, stop looking\n",
    "\n",
    "            if not course_outline_found:\n",
    "                print(f\"'COURSE OUTLINE' section not found in {os.path.basename(docx_file)}\")\n",
    "            elif not extracted_tables:\n",
    "                 print(f\"No target tables found in the 'COURSE OUTLINE' section of {os.path.basename(docx_file)}\")\n",
    "            else:\n",
    "                # --- Iterate through table data, Clean data with regex, and Store extracted data ---\n",
    "                print(f\"  Processing main course table with {len(extracted_tables['Main_Course_Table'].rows)} rows\")\n",
    "                table = extracted_tables[\"Main_Course_Table\"]\n",
    "                \n",
    "                for i, row in enumerate(table.rows):\n",
    "                    # Initialize a dictionary to hold data for the current row, including metadata\n",
    "                    row_data = {\n",
    "                        \"File\": os.path.basename(docx_file), # Store the original filename\n",
    "                        \"Table\": \"Course_Outline\", # Store the table name\n",
    "                        \"Row\": i + 1  # Add row number for reference\n",
    "                    }\n",
    "                    # Initialize a list for cleaned cell content for easy column creation later\n",
    "                    cleaned_cell_contents = []\n",
    "                    for j, cell in enumerate(row.cells):\n",
    "                        cell_text = cell.text\n",
    "\n",
    "                        # Use regex to replace bullet point characters with spaces instead of dashes\n",
    "                        cleaned_text_bullets = re.sub(r'[\\u2022\\u2023\\u2043\\u00B7]', ' ', cell_text)\n",
    "\n",
    "                        # Remove characters that can cause Excel formula errors completely\n",
    "                        # Remove equal signs, plus signs, at signs, and dashes to avoid ALL formula triggers\n",
    "                        cleaned_text_safe = re.sub(r'[=+@-]', ' ', cleaned_text_bullets)\n",
    "\n",
    "                        # Use regex to replace other special characters with spaces\n",
    "                        # This pattern keeps only letters, numbers, whitespace, periods, and commas\n",
    "                        cleaned_text_final = re.sub(r'[^a-zA-Z0-9\\s.,]', ' ', cleaned_text_safe)\n",
    "                        \n",
    "                        # Clean up multiple spaces and trim\n",
    "                        cleaned_text_final = re.sub(r'\\s+', ' ', cleaned_text_final).strip()\n",
    "\n",
    "                        # Special handling for Week column (first column) to prevent date conversion issues\n",
    "                        if j == 0 and i > 0:  # First column (Week) and not header row\n",
    "                            # Replace dashes with commas to prevent Excel date conversion (e.g., \"4-5\" -> \"4,5\")\n",
    "                            cleaned_text_final = re.sub(r'-', ',', cleaned_text_final)\n",
    "                            # Clean up periods at the end of week numbers (e.g., \"1.\" -> \"1\")\n",
    "                            cleaned_text_final = re.sub(r'^(\\d+)\\.', r'\\1', cleaned_text_final.strip())\n",
    "\n",
    "                        # Append the cleaned text to the list\n",
    "                        cleaned_cell_contents.append(cleaned_text_final)\n",
    "\n",
    "                    # Add the cleaned cell contents as columns to the row_data dictionary\n",
    "                    # Use the actual column headers from the first row if available\n",
    "                    if i == 0:  # This is the header row, store column names\n",
    "                        column_headers = cleaned_cell_contents\n",
    "                    else:  # Data rows\n",
    "                        for k, cleaned_content in enumerate(cleaned_cell_contents):\n",
    "                            if k < len(column_headers):\n",
    "                                # Use actual column name from header\n",
    "                                column_name = re.sub(r'[^a-zA-Z0-9_]', '_', column_headers[k].strip())\n",
    "                                if not column_name:\n",
    "                                    column_name = f\"Column_{k+1}\"\n",
    "                                \n",
    "                                # Skip the \"CO aligned to\" column\n",
    "                                if \"CO\" not in column_headers[k] or \"aligned\" not in column_headers[k]:\n",
    "                                    row_data[column_name] = cleaned_content\n",
    "                            else:\n",
    "                                row_data[f\"Column_{k+1}\"] = cleaned_content\n",
    "\n",
    "                        # Append the processed row data to the list for the current file (skip header row)\n",
    "                        current_file_extracted_data.append(row_data)\n",
    "\n",
    "            # After processing all tables in the current file, extend the combined list\n",
    "            all_files_combined_data.extend(current_file_extracted_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(docx_file)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responsible for processing the docx files and counting rows\n",
    "\n",
    "# Assuming all_files_combined_data is populated from the previous step where data for the current file was appended.\n",
    "# This step is implicitly handled by extending all_files_combined_data within the file processing loop\n",
    "# in the previous code block.\n",
    "# For clarity, we can add a print statement here to confirm the status of the combined data after the loop finishes.\n",
    "\n",
    "if 'all_files_combined_data' in locals() and all_files_combined_data:\n",
    "    print(f\"\\nFinished processing all files. Total combined rows extracted: {len(all_files_combined_data)}\")\n",
    "else:\n",
    "    print(\"\\nFinished processing all files. No combined data was extracted.\")\n",
    "\n",
    "# The all_files_combined_data list now holds the data from all processed files.\n",
    "# The next step will be to save this data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responsible for saving the combined data to a CSV file\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re # Need re for safe filename creation\n",
    "\n",
    "# Assuming all_files_combined_data is populated from the previous steps\n",
    "# Assuming output_folder is defined from a previous step\n",
    "\n",
    "if not all_files_combined_data:\n",
    "    print(\"No data was extracted. The combined CSV file will not be created.\")\n",
    "else:\n",
    "    print(\"Saving combined cleaned data to a single CSV file.\")\n",
    "    try:\n",
    "        # Create a pandas DataFrame from the list of dictionaries\n",
    "        df_combined = pd.DataFrame(all_files_combined_data)\n",
    "\n",
    "        # Define the output filename and path\n",
    "        output_filename = \"combined_cleaned_extracted_data.csv\"\n",
    "        # Ensure output_folder is defined, if not, use current directory as fallback\n",
    "        if 'output_folder' not in locals() or not os.path.exists(output_folder):\n",
    "             print(f\"Warning: output_folder not defined or does not exist. Saving to current directory: {output_filename}\")\n",
    "             output_filepath = output_filename\n",
    "        else:\n",
    "            output_filepath = os.path.join(output_folder, output_filename)\n",
    "\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df_combined.to_csv(output_filepath, index=False)\n",
    "        print(f\"Successfully saved the combined cleaned data to '{output_filepath}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the combined CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# Assuming docx_files is populated from a previous step\n",
    "# Assuming the loop to iterate through docx_files is in place\n",
    "# Assuming the code to find \"COURSE OUTLINE\", identify extracted_tables, process rows/cells,\n",
    "# and populate current_file_extracted_data for the current file is in place within the file loop.\n",
    "# Assuming output_folder is defined from a previous step.\n",
    "\n",
    "# For demonstration purposes, since no files were found in previous steps,\n",
    "# the following code will only execute if there were files to process and data was extracted for the current file.\n",
    "\n",
    "# Inside the loop processing each docx_file:\n",
    "# After the block that populates current_file_extracted_data for the current file:\n",
    "# Example structure (this code would conceptually follow the data extraction loop for a single file):\n",
    "# if current_file_extracted_data:\n",
    "#     # Create DataFrame, generate filename, and save CSV\n",
    "\n",
    "if not docx_files:\n",
    "    print(\"No DOCX files found. Cannot save individual CSVs.\")\n",
    "else:\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Processing file: {docx_file}\")\n",
    "        current_file_extracted_data = [] # This would be populated by previous steps\n",
    "\n",
    "        # --- Start of conceptual data extraction and cleaning for the current file ---\n",
    "        # This part is included conceptually to show where the saving logic fits.\n",
    "        # In the actual execution flow, this data would come from the previous steps.\n",
    "        try:\n",
    "            document = Document(docx_file)\n",
    "            course_outline_found = False\n",
    "            target_table_names = [\"Learning Outcomes\", \"Deliverables\", \"Assessment\", \"Instructional Materials\"]\n",
    "            extracted_tables = {}\n",
    "\n",
    "            # First, look for \"COURSE OUTLINE\" in paragraphs\n",
    "            for paragraph in document.paragraphs:\n",
    "                if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                    course_outline_found = True\n",
    "                    break\n",
    "\n",
    "            # If \"COURSE OUTLINE\" is found, look for the main course table\n",
    "            if course_outline_found:\n",
    "                for table in document.tables:\n",
    "                    if table.rows and table.rows[0].cells:\n",
    "                        # Get all text from the first row to analyze the table structure\n",
    "                        first_row_all_text = []\n",
    "                        for cell in table.rows[0].cells:\n",
    "                            first_row_all_text.append(cell.text.strip())\n",
    "                        \n",
    "                        # Check if this looks like the main course outline table (contains multiple target columns)\n",
    "                        targets_found_in_table = []\n",
    "                        for target_name in target_table_names:\n",
    "                            for cell_text in first_row_all_text:\n",
    "                                if target_name.lower() in cell_text.lower():\n",
    "                                    targets_found_in_table.append(target_name)\n",
    "                                    break\n",
    "                        \n",
    "                        if len(targets_found_in_table) >= 2:  # If table contains 2+ target columns\n",
    "                            extracted_tables[\"Main_Course_Table\"] = table\n",
    "                            break  # Found the main table, stop looking\n",
    "\n",
    "            if not course_outline_found or not extracted_tables:\n",
    "                print(f\"  No 'COURSE OUTLINE' section or target tables found in {os.path.basename(docx_file)}. Skipping saving.\")\n",
    "            else:\n",
    "                # Process the main course table (same logic as combined processing)\n",
    "                table = extracted_tables[\"Main_Course_Table\"]\n",
    "                print(f\"  Processing main course table with {len(table.rows)} rows\")\n",
    "                \n",
    "                for i, row in enumerate(table.rows):\n",
    "                    row_data = {\n",
    "                        \"File\": os.path.basename(docx_file),\n",
    "                        \"Table\": \"Course_Outline\",\n",
    "                        \"Row\": i + 1\n",
    "                    }\n",
    "                    cleaned_cell_contents = []\n",
    "                    for j, cell in enumerate(row.cells):\n",
    "                        cell_text = cell.text\n",
    "                        # Use regex to replace bullet point characters with spaces instead of dashes\n",
    "                        cleaned_text_bullets = re.sub(r'[\\u2022\\u2023\\u2043\\u00B7]', ' ', cell_text)\n",
    "                        \n",
    "                        # Remove characters that can cause Excel formula errors completely\n",
    "                        # Remove equal signs, plus signs, at signs, and dashes to avoid ALL formula triggers\n",
    "                        cleaned_text_safe = re.sub(r'[=+@-]', ' ', cleaned_text_bullets)\n",
    "\n",
    "                        # Use regex to replace other special characters with spaces\n",
    "                        # This pattern keeps only letters, numbers, whitespace, periods, and commas\n",
    "                        cleaned_text_final = re.sub(r'[^a-zA-Z0-9\\s.,]', ' ', cleaned_text_safe)\n",
    "                        \n",
    "                        # Clean up multiple spaces and trim\n",
    "                        cleaned_text_final = re.sub(r'\\s+', ' ', cleaned_text_final).strip()\n",
    "                        \n",
    "                        # Special handling for Week column (first column) to prevent date conversion issues\n",
    "                        if j == 0 and i > 0:  # First column (Week) and not header row\n",
    "                            # Replace dashes with commas to prevent Excel date conversion (e.g., \"4-5\" -> \"4,5\")\n",
    "                            cleaned_text_final = re.sub(r'-', ',', cleaned_text_final)\n",
    "                            # Clean up periods at the end of week numbers (e.g., \"1.\" -> \"1\")\n",
    "                            cleaned_text_final = re.sub(r'^(\\d+)\\.', r'\\1', cleaned_text_final.strip())\n",
    "                            \n",
    "                        cleaned_cell_contents.append(cleaned_text_final)\n",
    "\n",
    "                    # Use the actual column headers from the first row if available\n",
    "                    if i == 0:  # This is the header row, store column names\n",
    "                        column_headers = cleaned_cell_contents\n",
    "                    else:  # Data rows\n",
    "                        for k, cleaned_content in enumerate(cleaned_cell_contents):\n",
    "                            if k < len(column_headers):\n",
    "                                # Use actual column name from header\n",
    "                                column_name = re.sub(r'[^a-zA-Z0-9_]', '_', column_headers[k].strip())\n",
    "                                if not column_name:\n",
    "                                    column_name = f\"Column_{k+1}\"\n",
    "                                \n",
    "                                # Skip the \"CO aligned to\" column\n",
    "                                if \"CO\" not in column_headers[k] or \"aligned\" not in column_headers[k]:\n",
    "                                    row_data[column_name] = cleaned_content\n",
    "                            else:\n",
    "                                row_data[f\"Column_{k+1}\"] = cleaned_content\n",
    "\n",
    "                        # Append the processed row data to the list for the current file (skip header row)\n",
    "                        current_file_extracted_data.append(row_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during data extraction and cleaning for {os.path.basename(docx_file)}: {e}\")\n",
    "        # --- End of conceptual data extraction and cleaning for the current file ---\n",
    "\n",
    "\n",
    "        # --- Saving the data for the current file (Current Subtask) ---\n",
    "        if current_file_extracted_data:\n",
    "            print(f\"  Data extracted for {os.path.basename(docx_file)}. Preparing to save CSV.\")\n",
    "            try:\n",
    "                # Create a pandas DataFrame from the list of dictionaries for the current file\n",
    "                df_current_file = pd.DataFrame(current_file_extracted_data)\n",
    "\n",
    "                # Generate a safe output filename based on the original DOCX filename\n",
    "                original_filename_base = os.path.splitext(os.path.basename(docx_file))[0]\n",
    "                safe_filename_base = re.sub(r'[^\\w.-]', '_', original_filename_base)\n",
    "                output_filename = f\"cleaned_extracted_data_{safe_filename_base}.csv\"\n",
    "                # Ensure output_folder is defined and exists\n",
    "                if 'output_folder' not in locals() or not os.path.exists(output_folder):\n",
    "                    print(f\"Warning: output_folder not defined or does not exist ('{output_folder}'). Saving '{output_filename}' to current directory.\")\n",
    "                    output_filepath = output_filename\n",
    "                else:\n",
    "                    output_filepath = os.path.join(output_folder, output_filename)\n",
    "\n",
    "                # Save the DataFrame to a CSV file\n",
    "                df_current_file.to_csv(output_filepath, index=False)\n",
    "                print(f\"  Successfully saved cleaned data for {os.path.basename(docx_file)} to '{output_filepath}'\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving cleaned data for {os.path.basename(docx_file)} to CSV: {e}\")\n",
    "        else:\n",
    "            print(f\"No data extracted for {os.path.basename(docx_file)}. Skipping saving individual CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9545a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check Week column content in a few documents\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# Check the first 3 documents to see their Week column content\n",
    "debug_count = 0\n",
    "for docx_file in docx_files[:3]:  # Only check first 3 files\n",
    "    debug_count += 1\n",
    "    print(f\"\\nDEBUG FILE {debug_count}: {os.path.basename(docx_file)}\")\n",
    "    \n",
    "    try:\n",
    "        document = Document(docx_file)\n",
    "        course_outline_found = False\n",
    "        target_table_names = [\"Learning Outcomes\", \"Deliverables\", \"Assessment\", \"Instructional Materials\"]\n",
    "        \n",
    "        # Find COURSE OUTLINE section\n",
    "        for paragraph in document.paragraphs:\n",
    "            if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                course_outline_found = True\n",
    "                break\n",
    "        \n",
    "        if course_outline_found:\n",
    "            # Find the main table\n",
    "            for table in document.tables:\n",
    "                if table.rows and table.rows[0].cells:\n",
    "                    first_row_all_text = []\n",
    "                    for cell in table.rows[0].cells:\n",
    "                        first_row_all_text.append(cell.text.strip())\n",
    "                    \n",
    "                    # Check if this is the main course table\n",
    "                    targets_found = []\n",
    "                    for target_name in target_table_names:\n",
    "                        for cell_text in first_row_all_text:\n",
    "                            if target_name.lower() in cell_text.lower():\n",
    "                                targets_found.append(target_name)\n",
    "                                break\n",
    "                    \n",
    "                    if len(targets_found) >= 2:  # Found main table\n",
    "                        print(f\"Main table found with {len(table.rows)} rows\")\n",
    "                        print(f\"Headers: {first_row_all_text}\")\n",
    "                        \n",
    "                        # Show first few Week column values\n",
    "                        print(\"\\nWeek column content (first 10 rows):\")\n",
    "                        for i, row in enumerate(table.rows[:10]):  # First 10 rows\n",
    "                            if row.cells:\n",
    "                                week_content = row.cells[0].text.strip()\n",
    "                                if i == 0:\n",
    "                                    print(f\"  Row {i+1} (header): '{week_content}'\")\n",
    "                                else:\n",
    "                                    print(f\"  Row {i+1}: '{week_content}'\")\n",
    "                        break\n",
    "        else:\n",
    "            print(\"COURSE OUTLINE section not found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check which documents might not have main tables detected\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_files = []\n",
    "\n",
    "print(\"Checking all documents for main table detection:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for docx_file in docx_files:\n",
    "    filename = os.path.basename(docx_file)\n",
    "    try:\n",
    "        document = Document(docx_file)\n",
    "        course_outline_found = False\n",
    "        target_table_names = [\"Learning Outcomes\", \"Deliverables\", \"Assessment\", \"Instructional Materials\"]\n",
    "        main_table_found = False\n",
    "        \n",
    "        # Find COURSE OUTLINE section\n",
    "        for paragraph in document.paragraphs:\n",
    "            if \"COURSE OUTLINE\" in paragraph.text.strip().upper():\n",
    "                course_outline_found = True\n",
    "                break\n",
    "        \n",
    "        if course_outline_found:\n",
    "            # Check each table for main table characteristics\n",
    "            for table in document.tables:\n",
    "                if table.rows and table.rows[0].cells:\n",
    "                    first_row_all_text = []\n",
    "                    for cell in table.rows[0].cells:\n",
    "                        first_row_all_text.append(cell.text.strip())\n",
    "                    \n",
    "                    # Check if this is the main course table\n",
    "                    targets_found = []\n",
    "                    for target_name in target_table_names:\n",
    "                        for cell_text in first_row_all_text:\n",
    "                            if target_name.lower() in cell_text.lower():\n",
    "                                targets_found.append(target_name)\n",
    "                                break\n",
    "                    \n",
    "                    if len(targets_found) >= 2:  # Found main table\n",
    "                        main_table_found = True\n",
    "                        successful_extractions += 1\n",
    "                        print(f\"{filename}: Found main table with {len(table.rows)} rows\")\n",
    "                        break\n",
    "            \n",
    "            if not main_table_found:\n",
    "                failed_extractions += 1\n",
    "                failed_files.append(docx_file)\n",
    "                print(f\"{filename}: COURSE OUTLINE found but no main table detected\")\n",
    "        else:\n",
    "            failed_extractions += 1\n",
    "            failed_files.append(docx_file)\n",
    "            print(f\"{filename}: No COURSE OUTLINE section found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_extractions += 1\n",
    "        failed_files.append(docx_file)\n",
    "        print(f\"{filename}: Error - {e}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Summary:\")\n",
    "print(f\"Successful extractions: {successful_extractions}\")\n",
    "print(f\"Failed extractions: {failed_extractions}\")\n",
    "print(f\"Total files: {len(docx_files)}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFiled files ({len(failed_files)}):\")\n",
    "    for failed_file in failed_files[:10]:  # Show first 10\n",
    "        print(f\"  - {os.path.basename(failed_file)}\")\n",
    "    if len(failed_files) > 10:\n",
    "        print(f\"  ... and {len(failed_files) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae82ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all CSV files for Excel formula triggers\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "formula_triggers = ['=', '+', '-', '@']\n",
    "\n",
    "print(\"Checking CSV files for Excel formula triggers...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all CSV files in the output folder\n",
    "csv_files_in_output = [f for f in os.listdir(output_folder) if f.endswith('.csv')]\n",
    "\n",
    "for csv_filename in csv_files_in_output:\n",
    "    csv_path = os.path.join(output_folder, csv_filename)\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"\\n{csv_filename}:\")\n",
    "        try:\n",
    "            # Read the raw CSV content to check for formula triggers\n",
    "            with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            total_triggers = 0\n",
    "            for trigger in formula_triggers:\n",
    "                count = content.count(trigger)\n",
    "                if trigger == '-':\n",
    "                    # For minus signs, only count those at the beginning of cells (more dangerous)\n",
    "                    lines = content.split('\\n')\n",
    "                    dangerous_minus = 0\n",
    "                    for line in lines:\n",
    "                        cells = line.split(',')\n",
    "                        for cell in cells:\n",
    "                            if cell.strip().startswith('-') and len(cell.strip()) > 1:\n",
    "                                dangerous_minus += 1\n",
    "                    print(f\"   Leading '-': {dangerous_minus}\")\n",
    "                    total_triggers += dangerous_minus\n",
    "                else:\n",
    "                    print(f\"   '{trigger}': {count}\")\n",
    "                    total_triggers += count\n",
    "            \n",
    "            if total_triggers == 0:\n",
    "                print(\"   Clean - no formula triggers found\")\n",
    "            else:\n",
    "                print(f\"   Warning: {total_triggers} potential triggers found\")\n",
    "                \n",
    "            # Check total rows\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"   Rows: {len(df)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "    else:\n",
    "        print(f\"{csv_filename}: Not found\")\n",
    "\n",
    "print(\"\\nCheck complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
