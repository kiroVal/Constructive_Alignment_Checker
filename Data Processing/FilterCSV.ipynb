{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fuzzywuzzy python-levenshtein pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00881ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3329747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Set your folder paths here\n",
    "input_folder = 'C:\\\\REFACTOR\\\\docx_output'  # Where your cleaned CSV files are located\n",
    "output_folder = 'C:\\\\REFACTOR\\\\clean_output'  # Where filtered results will be saved\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Input folder: {input_folder}\")\n",
    "print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "# Load and explore CSV files\n",
    "csv_pattern = os.path.join(input_folder, 'cleaned_extracted_data_*.csv')\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "print(f\"\\nFound {len(csv_files)} CSV files to process:\")\n",
    "for i, file in enumerate(csv_files[:5], 1):  # Show first 5 files\n",
    "    print(f\"  {i}. {os.path.basename(file)}\")\n",
    "if len(csv_files) > 5:\n",
    "    print(f\"  ... and {len(csv_files) - 5} more files\")\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found! Please check the input folder path.\")\n",
    "else:\n",
    "    print(f\"\\nReady to process {len(csv_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Target Columns for Extraction\n",
    "target_columns_config = {\n",
    "    'learning_activities': {\n",
    "        'output_name': 'Learning_Activities',\n",
    "        'keywords': ['Learning_Activities', 'learning activities', 'activities', 'learning activity', 'activity']\n",
    "    },\n",
    "    'learning_outcomes': {\n",
    "        'output_name': 'Learning_Outcomes_LO_Learner_and_Learning_Outcomes_LLO',\n",
    "        'keywords': ['Learning_Outcomes_LO_Learner_and_Learning_Outcomes_LLO', 'learning outcomes', 'outcomes', 'learning outcome', 'LO', 'LLO']\n",
    "    },\n",
    "    'assessment': {\n",
    "        'output_name': 'Assessment',\n",
    "        'keywords': ['Assessment', 'assessment', 'assessments', 'evaluation', 'grade', 'grading']\n",
    "    }\n",
    "}\n",
    "\n",
    "similarity_threshold = 60  # Minimum similarity score for fuzzy matching (0-100)\n",
    "\n",
    "print(\"Target Columns Configuration:\")\n",
    "for key, config in target_columns_config.items():\n",
    "    print(f\"   {config['output_name']}:\")\n",
    "    print(f\"     Keywords: {', '.join(config['keywords'])}\")\n",
    "print(f\"\\nSimilarity threshold: {similarity_threshold}% (minimum match confidence)\")\n",
    "\n",
    "# Explore sample CSV structure\n",
    "if csv_files:\n",
    "    print(f\"\\nSample CSV Structure (from {os.path.basename(csv_files[0])}):\")\n",
    "    try:\n",
    "        sample_df = pd.read_csv(csv_files[0])\n",
    "        print(f\"Rows: {len(sample_df)}\")\n",
    "        print(f\"Columns: {len(sample_df.columns)}\")\n",
    "        print(\"Column names:\")\n",
    "        for i, col in enumerate(sample_df.columns, 1):\n",
    "            print(f\"     {i:2d}. {col}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error reading sample file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Cleaning Functions\n",
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Calculate similarity between two strings using multiple ML methods\"\"\"\n",
    "    # Normalize strings - remove special characters and convert to lowercase\n",
    "    text1_norm = re.sub(r'[^a-zA-Z0-9]', '', str(text1).lower())\n",
    "    text2_norm = re.sub(r'[^a-zA-Z0-9]', '', str(text2).lower())\n",
    "    \n",
    "    if not text1_norm or not text2_norm:\n",
    "        return 0\n",
    "    \n",
    "    # Multiple similarity metrics for robust matching\n",
    "    ratio = SequenceMatcher(None, text1_norm, text2_norm).ratio() * 100\n",
    "    fuzzy_ratio = fuzz.ratio(text1_norm, text2_norm)\n",
    "    fuzzy_partial = fuzz.partial_ratio(text1_norm, text2_norm)\n",
    "    fuzzy_token = fuzz.token_sort_ratio(text1_norm, text2_norm)\n",
    "    \n",
    "    # Return the maximum similarity score\n",
    "    return max(ratio, fuzzy_ratio, fuzzy_partial, fuzzy_token)\n",
    "\n",
    "def find_best_column_match(available_columns, target_config):\n",
    "    \"\"\"Use ML techniques to find the best matching column\"\"\"\n",
    "    keywords = target_config['keywords']\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for col in available_columns:\n",
    "        col_clean = str(col).strip()\n",
    "        if not col_clean or col_clean.lower() in ['unnamed', 'column', 'nan']:\n",
    "            continue\n",
    "            \n",
    "        # Calculate similarity against all target keywords\n",
    "        max_similarity = 0\n",
    "        for keyword in keywords:\n",
    "            similarity = calculate_similarity(col_clean, keyword)\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        # Update best match if this column has higher similarity\n",
    "        if max_similarity > best_score and max_similarity >= similarity_threshold:\n",
    "            best_score = max_similarity\n",
    "            best_match = col_clean\n",
    "    \n",
    "    return best_match, best_score\n",
    "\n",
    "def clean_cell_data(value):\n",
    "    \"\"\"Clean individual cell data\"\"\"\n",
    "    if pd.isna(value) or value == '' or str(value).lower() in ['nan', 'none']:\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    cleaned = str(value).strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"Functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and Filter Individual CSV Files\n",
    "def process_single_csv(csv_path):\n",
    "    \"\"\"Process a single CSV file and extract target columns\"\"\"\n",
    "    filename = os.path.basename(csv_path)\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"   Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "        # Find best matching columns using ML\n",
    "        matched_columns = {}\n",
    "        detection_scores = {}\n",
    "        \n",
    "        for target_type, config in target_columns_config.items():\n",
    "            best_match, score = find_best_column_match(df.columns, config)\n",
    "            if best_match:\n",
    "                matched_columns[target_type] = best_match\n",
    "                detection_scores[target_type] = score\n",
    "                print(f\"   {config['output_name']}: '{best_match}' (confidence: {score:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"   {config['output_name']}: No suitable match found\")\n",
    "        \n",
    "        # Extract and clean data\n",
    "        extracted_data = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_data = {\n",
    "                'File': filename,\n",
    "                'Row': index + 1\n",
    "            }\n",
    "            \n",
    "            # Extract each target column if found\n",
    "            for target_type, config in target_columns_config.items():\n",
    "                output_name = config['output_name']\n",
    "                \n",
    "                if target_type in matched_columns:\n",
    "                    original_value = row[matched_columns[target_type]]\n",
    "                    cleaned_value = clean_cell_data(original_value)\n",
    "                    row_data[output_name] = cleaned_value\n",
    "                else:\n",
    "                    row_data[output_name] = ''  # Empty if column not found\n",
    "            \n",
    "            extracted_data.append(row_data)\n",
    "        \n",
    "        print(f\"   Extracted {len(extracted_data)} rows with {len(matched_columns)}/3 target columns\")\n",
    "        return extracted_data, matched_columns, detection_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error processing: {e}\")\n",
    "        return [], {}, {}\n",
    "\n",
    "# Initialize storage for all extracted data\n",
    "all_filtered_data = []\n",
    "processing_stats = {\n",
    "    'files_processed': 0,\n",
    "    'files_with_all_columns': 0,\n",
    "    'files_with_partial_columns': 0,\n",
    "    'total_rows_extracted': 0,\n",
    "    'column_detection_details': {}\n",
    "}\n",
    "\n",
    "print(\"Starting individual CSV processing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b281f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process All CSV Files and Save Individual Filtered Files\n",
    "for csv_file in csv_files:\n",
    "    # Process the CSV file\n",
    "    extracted_data, matched_columns, detection_scores = process_single_csv(csv_file)\n",
    "    \n",
    "    if extracted_data:\n",
    "        # Store data for combined processing\n",
    "        all_filtered_data.extend(extracted_data)\n",
    "        \n",
    "        # Save individual filtered CSV\n",
    "        filename = os.path.basename(csv_file)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Remove 'cleaned_extracted_data_' prefix if present\n",
    "        if base_name.startswith('cleaned_extracted_data_'):\n",
    "            base_name = base_name.replace('cleaned_extracted_data_', '')\n",
    "        \n",
    "        # Create filtered filename\n",
    "        filtered_filename = f\"filtered_extracted_data_{base_name}.csv\"\n",
    "        output_path = os.path.join(output_folder, filtered_filename)\n",
    "        \n",
    "        try:\n",
    "            # Save filtered data\n",
    "            df_filtered = pd.DataFrame(extracted_data)\n",
    "            df_filtered.to_csv(output_path, index=False)\n",
    "            print(f\"   Saved: {filtered_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error saving: {e}\")\n",
    "        \n",
    "        # Update statistics\n",
    "        columns_found = len(matched_columns)\n",
    "        if columns_found == 3:\n",
    "            processing_stats['files_with_all_columns'] += 1\n",
    "        elif columns_found > 0:\n",
    "            processing_stats['files_with_partial_columns'] += 1\n",
    "            \n",
    "        processing_stats['total_rows_extracted'] += len(extracted_data)\n",
    "        processing_stats['column_detection_details'][filename] = detection_scores\n",
    "    \n",
    "    processing_stats['files_processed'] += 1\n",
    "\n",
    "print(\"Individual CSV processing completed!\")\n",
    "print(f\"Files processed: {processing_stats['files_processed']}\")\n",
    "print(f\"Total rows extracted: {processing_stats['total_rows_extracted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine All Filtered Data\n",
    "print(\"\\nCreating combined filtered dataset...\")\n",
    "\n",
    "if all_filtered_data:\n",
    "    try:\n",
    "        # Create combined DataFrame\n",
    "        df_combined = pd.DataFrame(all_filtered_data)\n",
    "        \n",
    "        # Save combined filtered CSV\n",
    "        combined_path = os.path.join(output_folder, 'combined_filtered_extracted_data.csv')\n",
    "        df_combined.to_csv(combined_path, index=False)\n",
    "        \n",
    "        print(\"Saved: combined_filtered_extracted_data.csv\")\n",
    "        print(f\"Total rows: {len(df_combined)}\")\n",
    "        print(f\"Columns: {list(df_combined.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating combined file: {e}\")\n",
    "        df_combined = pd.DataFrame()\n",
    "else:\n",
    "    print(\"No data to combine!\")\n",
    "    df_combined = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Column-Specific Combined Files\n",
    "import time\n",
    "execution_id = int(time.time() * 1000) % 10000  # Simple execution tracker\n",
    "print(f\"Creating column-specific combined files... (exec: {execution_id})\")\n",
    "\n",
    "if not df_combined.empty:\n",
    "    # 1. All Learning Activities\n",
    "    learning_activities_data = []\n",
    "    for _, row in df_combined.iterrows():\n",
    "        value = str(row['Learning_Activities']).strip() if pd.notna(row['Learning_Activities']) else ''\n",
    "        if value:  # Only include non-empty entries\n",
    "            learning_activities_data.append({\n",
    "                'File': row['File'],\n",
    "                'Row': row['Row'],\n",
    "                'Learning_Activities': value\n",
    "            })\n",
    "    \n",
    "    if learning_activities_data:\n",
    "        df_activities = pd.DataFrame(learning_activities_data)\n",
    "        # Remove duplicates based on content (keep first occurrence)\n",
    "        df_activities = df_activities.drop_duplicates(subset=['Learning_Activities'], keep='first')\n",
    "        activities_path = os.path.join(output_folder, 'all_learning_activities.csv')\n",
    "        df_activities.to_csv(activities_path, index=False)\n",
    "        print(f\"Saved: all_learning_activities.csv ({len(df_activities)} entries)\")\n",
    "    \n",
    "    # 2. All Learning Outcomes\n",
    "    learning_outcomes_data = []\n",
    "    for _, row in df_combined.iterrows():\n",
    "        value = str(row['Learning_Outcomes_LO_Learner_and_Learning_Outcomes_LLO']).strip() if pd.notna(row['Learning_Outcomes_LO_Learner_and_Learning_Outcomes_LLO']) else ''\n",
    "        if value:  # Only include non-empty entries\n",
    "            learning_outcomes_data.append({\n",
    "                'File': row['File'],\n",
    "                'Row': row['Row'],\n",
    "                'Learning_Outcomes_LO_Learner_and_Learning_Outcomes_LLO': value\n",
    "            })\n",
    "    \n",
    "    if learning_outcomes_data:\n",
    "        df_outcomes = pd.DataFrame(learning_outcomes_data)\n",
    "        # Remove duplicates based on content (keep first occurrence)\n",
    "        df_outcomes = df_outcomes.drop_duplicates(subset=['Learning_Outcomes_LO_Learner_and_Learning_Outcomes_LLO'], keep='first')\n",
    "        outcomes_path = os.path.join(output_folder, 'all_learning_outcomes.csv')\n",
    "        df_outcomes.to_csv(outcomes_path, index=False)\n",
    "        print(f\"Saved: all_learning_outcomes.csv ({len(df_outcomes)} entries)\")\n",
    "    \n",
    "    # 3. All Assessments\n",
    "    assessment_data = []\n",
    "    for _, row in df_combined.iterrows():\n",
    "        value = str(row['Assessment']).strip() if pd.notna(row['Assessment']) else ''\n",
    "        if value:  # Only include non-empty entries\n",
    "            assessment_data.append({\n",
    "                'File': row['File'],\n",
    "                'Row': row['Row'],\n",
    "                'Assessment': value\n",
    "            })\n",
    "    \n",
    "    if assessment_data:\n",
    "        df_assessments = pd.DataFrame(assessment_data)\n",
    "        # Remove duplicates based on content (keep first occurrence)\n",
    "        df_assessments = df_assessments.drop_duplicates(subset=['Assessment'], keep='first')\n",
    "        assessments_path = os.path.join(output_folder, 'all_assessment.csv')\n",
    "        df_assessments.to_csv(assessments_path, index=False)\n",
    "        print(f\"Saved: all_assessment.csv ({len(df_assessments)} entries)\")\n",
    "        \n",
    "else:\n",
    "    print(\"No combined data available for column-specific files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
